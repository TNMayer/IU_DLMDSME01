{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab95ffc0-5989-4435-b12d-8481711c61e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jenkspy import JenksNaturalBreaks\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import seaborn as sns\n",
    "import itertools\n",
    "import sqlite3\n",
    "\n",
    "import modules.feature_selection as fs\n",
    "import modules.helper_functions as hf\n",
    "import modules.filter_rows as fr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a8abae-bdbe-451f-ab9b-a524775a4d5f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c903743d-f1af-478b-b391-54dd436d01f2",
   "metadata": {},
   "source": [
    "All helper functions are included in the module ```helper_functions.py```, such that they can also be used in other notebooks as well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0a8935-84e8-4720-b36b-257916d4960e",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cff96cc-dd0c-422e-8e33-455857f2f0f3",
   "metadata": {},
   "source": [
    "According to IBM corporation (2013) the data preparation process can be outlined as follows (p. 18):\n",
    "\n",
    "The data preparation phase covers all activities to construct the final dataset (data that will be fed into the modeling tool(s)) from the initial raw data. Data preparation tasks are likely to be performed multiple times, and not in any prescribed order. Tasks include table, record, and attribute selection, data cleaning, construction of new attributes, and transformation of data for modeling tools.\n",
    "\n",
    "Data preparation is one of the most important and often time-consuming aspects of data mining. In fact, it is estimated that data preparation usually takes 50-70% of a project's time and effort. Devoting adequate energy to the earlier business understanding and data understanding phases can minimize this overhead, but you still need to expend a good amount of effort preparing and packaging the data for mining. Depending on your organization and its goals, data preparation typically involves the following tasks: \n",
    "* Merging data sets and/or records \n",
    "* Selecting a sample subset of data\n",
    "* Aggregating records \n",
    "* Deriving new attributes \n",
    "* Sorting the data for modeling \n",
    "* Removing or replacing blank or missing values \n",
    "* Splitting into training and test data sets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926e5056-03af-4694-b794-5eb6f0241b87",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Select Data\n",
    "\n",
    "The data selection phase contains the selection of rows and columns which are necessary for the further modeling process. For the case-study it is first of all important to address the given insights from the business side and keep only distinct purchases in the dataset. This means we have to remove all transactions where customers tried several times to transfer the money. If two transactions are within one minute, with the same amount of money and from the same country, it is (for a decent number of tries) safe to assume that they are payment attempts of the same purchase. This means we have to remove those unsuccessful transactions from the dataset, who meat the previously stated equality-criteria from business side."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d34c473-e5d9-45aa-b619-7d7843d4ffec",
   "metadata": {},
   "source": [
    "### Rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894a2b73-e0ad-45e5-a8c0-dea956a5fcd6",
   "metadata": {},
   "source": [
    "All functions needed to select the rows are stored in the module ```./modules/filter_rows.py```."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42af7101-4a49-4bfc-896e-a6d3ecff511f",
   "metadata": {},
   "source": [
    "### Columnms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066b4100-1f5a-4ec9-8fb7-382076158e47",
   "metadata": {},
   "source": [
    "All functions needed to drop columns are stored in the module ```./modules/helper_functions.py```."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ad847d-bca6-49e1-a947-5050bb078bf3",
   "metadata": {},
   "source": [
    "### Feature selection\n",
    "Because CRISP-DM is a cyclic process we reference to a subsequent process back namely the formatting step. <br>\n",
    "In this step we first have to load the needed and splitted data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d408cdf-0d4f-4fa0-a6e3-7179a67b17f6",
   "metadata": {},
   "source": [
    "In terms of feature importance it is almost anytime important to remove redundant features which are highly correlated with each other. We have binary variables and continuous variables in the dataset. For the correlation of two continuous variables the correlation is calculated based on the Pearson correlation coefficient, for two binary variables it is the Phi-coefficient and for a binary and a continuous variable it is the point biseral correlation. The Phi-coefficient and the point biseral correlation are both special cases of the Pearson correlation. Hence, for all feature constellations in the dataset we can calculate the Pearson correlation coefficient in order to remove redundant features. Kuhn and Johnson (2013, p. 47) propose a pairwise between-predictor correlation of less than 0.75 for models that are particularly sensitive to multicollinearity. In order to find those predictors Kuhn and Johnson propose the following algorithm:\n",
    "\n",
    "***\n",
    "**Correlation based feature selection (Kuhn & Johnson, 2013)**\n",
    "***\n",
    "**Input**: training matrix, threshold\n",
    "* Calculate the correlation matrix of the predictors\n",
    "* Determine the two predictors associated with the largest absolute pairwise correlation (A and B)\n",
    "* Determine the average correlation between A and the other variables. Do the same for predictor B.\n",
    "* If A has a larger average correlation, remove it, otherwise remove B.\n",
    "* Repeat the steps until no absolute correlations are above the threshold.\n",
    "\n",
    "**Output**: List of featurs that have a higher between-predictor correlation than the specified threshold\n",
    "***\n",
    "\n",
    "The algorithm is applied in the function ```correlationFiltering(X_train, threshold = 0.75)```. There are no predictors in the dataset with a higher pairwise correlation than 0.75.\n",
    "\n",
    "Based on the model a second round will be implemented in the modeling part. A model-based wrapper approach is planned as a second feature selection step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05cd9122-cf79-48f9-a319-eb92a2ee4bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlationFiltering(X_train, threshold = 0.75, figsize = 10):\n",
    "    plt.figure(figsize=(figsize, figsize))\n",
    "    sns.heatmap(X_train.corr().round(2), annot=False)\n",
    "    plt.show()\n",
    "    \n",
    "    # create correlation matrix\n",
    "    corrMatrix = X_train.corr().abs()\n",
    "    # get upper triangle\n",
    "    upperCorrMatrix = corrMatrix.where(\n",
    "        np.triu(np.ones(corrMatrix.shape), k=1).astype(np.bool_))\n",
    "    uniqueCorrPairs = upperCorrMatrix.unstack().dropna()\n",
    "    sortedCorrPairs = uniqueCorrPairs.sort_values(ascending = False)\n",
    "    # identify all paird with correlation greater than threshold\n",
    "    pairsToFilter = sortedCorrPairs[sortedCorrPairs > threshold]\n",
    "    toRemove = []\n",
    "    for pair in pairsToFilter.index:\n",
    "        # calculate average correlation between A and other variables and B with other variables\n",
    "        a = pair[0]\n",
    "        a_avg = corrMatrix[a].mean()\n",
    "        b = pair[1]\n",
    "        b_avg = corrMatrix[b].mean()\n",
    "        # if A has a larger average correlation, remove it, otherwise remove B\n",
    "        if a_avg > b_avg:\n",
    "            toRemove.append(a)\n",
    "        else:\n",
    "            toRemove.append(b)\n",
    "\n",
    "    return list(set(toRemove))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0623ec99-75a3-43e9-86c0-d58fe1f6dc47",
   "metadata": {},
   "source": [
    "This function is also transferred to the module ```feature_selection.py```, such that it can be used in subsequent modeling steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c308ebb7-287e-4269-98da-2d166882016e",
   "metadata": {},
   "source": [
    "## Clean Data\n",
    "**Data Cleaning Report** <br>\n",
    "Cleaning your data involves taking a closer look at the problems in the data that you've chosen to include for analysis. There are several ways to clean data using the Record and Field Operation nodes in IBM SPSS Modeler. Common data problems are:\n",
    "* Missing data: Exclude rows or characteristics. Or, fill blanks with an estimated value.\n",
    "* Data errors: Use logic to manually discover errors and replace. Or, exclude characteristics.\n",
    "* Coding inconsistencies: Decide upon a single coding scheme, then convert and replace values.\n",
    "* Missing or bad metadata: Manually examine suspect fields and track down correct meaning.\n",
    "\n",
    "The Data Quality Report prepared during the data understanding phase contains details about the types of problems particular to your data. You can use it as a starting point for data manipulation in IBM SPSS Modeler.\n",
    "\n",
    "## Construct Data\n",
    "**Derived Attributes Generated Record** <br>\n",
    "It is frequently the case that you'll need to construct new data. For example, it may be useful to create a new column flagging the purchase of an extended warranty for each transaction. This new field, purchased_warranty, can easily be generated using a Set to Flag node in IBM SPSS Modeler. There are two ways to construct new data: \n",
    "* Deriving attributes (columns or characteristics) \n",
    "* Generating records (rows) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d56aebf-3863-4679-95bf-98b9ba8482c2",
   "metadata": {},
   "source": [
    "## Integrate Data\n",
    "**Merged Data** <br>\n",
    "It is not uncommon to have multiple data sources for the same set of business questions. For example, you may have access to mortgage loan data as well as purchased demographic data for the same set of clients. If these data sets contain the same unique identifier (such as social security number), you can merge them in IBM SPSS Modeler using this key field. There are two basic methods of integrating data:\n",
    "* Merging data involves merging two data sets with similar records but different attributes. The data is merged using the same key identifier for each record (such as customer ID). The resulting data increases in columns or characteristics. \n",
    "* Appending data involves integrating two or more data sets with similar attributes but different records. The data is integrated based upon a similar fields (such as product name or contract length)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "effe04ae-dc65-45ae-af85-d406e4645ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tmsp_information(data):\n",
    "    out = data.copy()\n",
    "    out[\"month\"] = out.copy()[\"tmsp\"].dt.strftime('%b')\n",
    "    out[\"dayOfMonth\"] = out.copy()[\"tmsp\"].dt.strftime('%#d').astype(int)\n",
    "    out[\"weekday\"] = out.copy().tmsp.dt.day_name()\n",
    "    out[\"weekend\"] = np.where(out['weekday'].isin(['Saturday', 'Sunday']), 1, 0)\n",
    "    out[\"holiday\"] = np.where((out['month'] == 'Jan') & (out['dayOfMonth'] == 1), 1, 0)\n",
    "    \n",
    "    return out\n",
    "\n",
    "def get_daytime(data):\n",
    "    out = data.copy()\n",
    "    \n",
    "    out['time'] = out['tmsp'].dt.strftime('%H:%M')\n",
    "    out['daytime'] = np.where((out['time'] >= '00:00') & (out['time'] < '06:00'), 'night', \n",
    "                        np.where((out['time'] >= '06:00') & (out['time'] < '12:00'), 'morning',\n",
    "                        np.where((out['time'] >= '12:00') & (out['time'] < '18:00'), 'afternoon', 'evening')))\n",
    "    out[\"minuteOfDay\"] = (out[\"tmsp\"].dt.hour * 60) + (out[\"tmsp\"].dt.minute)\n",
    "    \n",
    "    return out\n",
    "\n",
    "def get_amountgroup(data, train_length = 0.7, on_training = True):\n",
    "    out = data.copy()\n",
    "    out = out.sort_values(by = [\"tmsp\"], ascending = True)\n",
    "    out_length = len(out)\n",
    "    parameters = {}\n",
    "    \n",
    "    if on_training:\n",
    "        train_split = int(np.round(out_length*train_length))\n",
    "        parameters['train_split_iloc'] = train_split\n",
    "    else:\n",
    "        train_split = out_length\n",
    "    \n",
    "    amount = list(out.iloc[:train_split,:]['amount'])\n",
    "    \n",
    "    jnb = JenksNaturalBreaks(5)\n",
    "    jnb.fit(amount)\n",
    "    bins = jnb.breaks_\n",
    "    bins[0] = 0\n",
    "    bins[len(bins) - 1] = 10000\n",
    "    parameters[\"jenks\"] = bins\n",
    "    hf.writePickle('./data/parameters.pkl', parameters)\n",
    "    \n",
    "    out = hf.get_amountgroups(out, bins = bins)\n",
    "    \n",
    "    print(\"= Jenks natural breaks are:\")\n",
    "    print(jnb.breaks_)\n",
    "    \n",
    "    return out\n",
    "\n",
    "def getCard_3DSec_PSP_Amountgroup(data):\n",
    "    y = hf.get_y(groups=[\"card\", \"3D_secured\", \"PSP\", \"amountgroup_word\"], data = data.copy())\n",
    "    y = y.sort_values(by=['success_rate'], ascending = False)\n",
    "    y_mod = y.drop('PSP', axis=1)\n",
    "    y_mod = y_mod.groupby([\"card\", \"3D_secured\", \"amountgroup_word\"]).aggregate({'success_rate': 'max'}).reset_index()\n",
    "    y_mod = y_mod.sort_values(by=[\"success_rate\"], ascending = False)\n",
    "    y_mod = y_mod.merge(y, how=\"left\", on = [\"card\", \"3D_secured\", \"amountgroup_word\", \"success_rate\"])\n",
    "\n",
    "    return y_mod"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208a5ff3-9d70-428d-be8c-b3fc78eac172",
   "metadata": {},
   "source": [
    "The success rate which is the most relevant optimization criterion hasn´t been explored yet to generate features for the machine learning model. It is not possible to just calculate the overall success rate for different attributes (e.g. PSP) because this would imply data leakage. Those features have to be calculated in a rolling window approach based on previous transactions. In order to not throw away data missing values will be imputed if there is no data based on the most probable success overall or for a given PSP. The overall success rate is 0.37. The success rate for Goldcard is 0.62, for UK Card 0.4, for Moneycard 0.37 and for Simplecard 0.26. So the missing data imputation which is only relevant for observations at the very beginning of the dataset is the imputation of 0 by default and 1 for the PSP Goldcard. But this case should be limited to just a few observations. In order to apply this rolling success rate calculation the dataset has to be ordered based on the timestamp which we already did."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "298dbb24-e86a-4927-819e-f5720f28f493",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getOverallSR(data):\n",
    "    out = data.copy()\n",
    "    train_split = hf.loadPickle('./data/parameters.pkl')[\"train_split_iloc\"]\n",
    "    out[\"overallSR\"] = out.iloc[:train_split, :].success.mean()\n",
    "                \n",
    "    return out\n",
    "\n",
    "def combinatoric_SR(data, addColumns = [\"PSP\", \"card\", \"3D_secured\", \"amountgroup_word\"]):\n",
    "    out = data.copy()\n",
    "\n",
    "    train_split = hf.loadPickle('./data/parameters.pkl')[\"train_split_iloc\"]\n",
    "    combinations = {}\n",
    "    colName = \"\"\n",
    "    for col in addColumns:\n",
    "        combinations[col] = list(out[col].unique())\n",
    "        colName = colName + col + \"_\"\n",
    "\n",
    "    colName = colName + \"SR\"\n",
    "    print(colName)\n",
    "    addColumns.append(colName)\n",
    "\n",
    "    keys, values = zip(*combinations.items())\n",
    "    permutations_dicts = [dict(zip(keys, v)) for v in itertools.product(*values)]\n",
    "\n",
    "    joinFrame = pd.DataFrame()\n",
    "\n",
    "    i = 1\n",
    "    for permutation in permutations_dicts:\n",
    "        subset = out.copy().iloc[:train_split, :]\n",
    "        for key in permutation.keys():\n",
    "            subset = subset[subset[key] == permutation[key]]\n",
    "        subset[colName] = subset.success.mean()\n",
    "        joinFrame = pd.concat([joinFrame, subset[addColumns]])\n",
    "\n",
    "    out = out.merge(joinFrame.drop_duplicates(), how = 'left', on = list(set(out.columns).intersection(set(joinFrame.columns))))\n",
    "    \n",
    "    return out\n",
    "\n",
    "def combinatoric_event_window_SR(data, \n",
    "                                 addColumns = [\"PSP\", \"card\", \"3D_secured\", \"amountgroup_word\"], \n",
    "                                 event_windows = [5, 10, 100, 200],\n",
    "                                 allowed_missing = 0.05\n",
    "                                ):\n",
    "    out = data.copy()\n",
    "\n",
    "    for event_window in event_windows:\n",
    "        print(\"= Event window size: \" + str(event_window))\n",
    "        combinations = {}\n",
    "        colName = \"\"\n",
    "        replaceCol = \"\"\n",
    "        for col in addColumns:\n",
    "            combinations[col] = list(out[col].unique())\n",
    "            colName = colName + col + \"_\"\n",
    "            replaceCol = replaceCol + col + \"_\"\n",
    "\n",
    "        colName = colName + \"e\" + str(event_window) + \"_SR\"\n",
    "        replaceCol = replaceCol + \"SR\"\n",
    "        print(colName)\n",
    "        outCols = addColumns.copy()\n",
    "        outCols.append(colName)\n",
    "\n",
    "        keys, values = zip(*combinations.items())\n",
    "        permutations_dicts = [dict(zip(keys, v)) for v in itertools.product(*values)]\n",
    "\n",
    "        joinFrame = pd.DataFrame()\n",
    "\n",
    "        for permutation in permutations_dicts:\n",
    "            subset = out.copy()\n",
    "            for key in permutation.keys():\n",
    "                subset = subset[subset[key] == permutation[key]]\n",
    "            subset[colName] = subset.success.shift().rolling(\n",
    "                event_window, min_periods=int(np.ceil(event_window/10))\n",
    "            ).mean()\n",
    "            joinFrame = pd.concat([joinFrame, subset[outCols]])\n",
    "        \n",
    "        missing_ratio = joinFrame.isna().sum().sum()/len(joinFrame)\n",
    "        if missing_ratio <= allowed_missing:\n",
    "            out = out.join(joinFrame[colName])\n",
    "            out[colName] = out[colName].fillna(out[replaceCol])\n",
    "        else:\n",
    "            print(\"--- Number of missing values too large ---\")\n",
    "    \n",
    "    return out\n",
    "\n",
    "def combinatoric_time_window_SR(data,\n",
    "                                addColumns = [\"PSP\", \"card\", \"3D_secured\", \"amountgroup_word\"], \n",
    "                                time_windows = [1, 6, 12, 24, 72],\n",
    "                                allowed_missing = 0.15\n",
    "                                ):\n",
    "    out = data.copy()\n",
    "\n",
    "    for time_window in time_windows:\n",
    "        print(\"= Time window size: \" + str(time_window) + \"h\")\n",
    "        combinations = {}\n",
    "        colName = \"\"\n",
    "        replaceCol = \"\"\n",
    "        for col in addColumns:\n",
    "            combinations[col] = list(out[col].unique())\n",
    "            colName = colName + col + \"_\"\n",
    "            replaceCol = replaceCol + col + \"_\"\n",
    "\n",
    "        colName = colName + \"t\" + str(time_window) + \"h_SR\"\n",
    "        replaceCol = replaceCol + \"SR\"\n",
    "        print(colName)\n",
    "        outCols = addColumns.copy()\n",
    "        outCols.append(colName)\n",
    "\n",
    "        keys, values = zip(*combinations.items())\n",
    "        permutations_dicts = [dict(zip(keys, v)) for v in itertools.product(*values)]\n",
    "\n",
    "        joinFrame = pd.DataFrame()\n",
    "\n",
    "        for permutation in permutations_dicts:\n",
    "            subset = out.copy()\n",
    "            for key in permutation.keys():\n",
    "                subset = subset[subset[key] == permutation[key]]\n",
    "            subset[colName] = subset[[\"tmsp\", \"success\"]].rolling(\n",
    "                            str(time_window) + \"h\", on = \"tmsp\", min_periods=int(np.min([np.round(time_window/10), 3]))\n",
    "                        ).apply(hf.getMeanRollingEvent)[\"success\"]\n",
    "            joinFrame = pd.concat([joinFrame, subset[outCols]])\n",
    "\n",
    "        missing_ratio = joinFrame.isna().sum().sum()/len(joinFrame)\n",
    "        if missing_ratio <= allowed_missing:\n",
    "            out = out.join(joinFrame[colName])\n",
    "            out[colName] = out[colName].fillna(out[replaceCol])\n",
    "        else:\n",
    "            print(\"--- Number of missing values too large: \" + str(missing_ratio) + \" ---\")\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b427336c-4c12-4fd3-97ad-31796f12bb30",
   "metadata": {},
   "source": [
    "Bygari, et al. (2021) developed a similar routing approach for an India-based payment service provider called Razorpay. Instead of different payment service providers the usecase has several terminals. For the described \"Smart Routing Solution\" the authors also proposed to calculate the success rates based on different event- and time-windows. This means that the success rates for a transaction are also calculated based on a rolling-window approach. This means for time-windows:\n",
    "* The success rates are calculated based on the transaction-access in the last t seconds. This is done for all transactions in the time-wondow as well as for each payment service provider separately. This means concretely, if the timestamp for a transaction is T the success rates are calculated for the transactions between [T-t, T). Even though Bygari, et al. (2021) described a greedy approach to find the best suited timeintervals with a wide range of t´s, the t´s for this case-study will be limited to [300, 600, 3600] seconds. If a time-interval does not contain any transaction the missing values are imputed by the rolling overall success rate. Furthermore the number of minimum transactions to calculate the success rate is set to one.\n",
    "\n",
    "This means for event-windows:\n",
    "* This means for the rolling-event approach is calculated simularly. In this case t is replaced with e. This means that the success rate for a certain transaction is calculated for the e previous transactions. For this case-study e will be limited to [5, 10, 50] events. The number of minimum transactions is also set to one.\n",
    "\n",
    "Furthermore all window-based feature engineering approaches can be computed overall and based on different features like ```PSP```, ```country``` or ```card```. This will be limited to the column ```PSP``` for this case-study."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f29d4fd2-76c4-4187-88d8-b442751038a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_raw_data(dataPath = './data/PSP_Jan_Feb_2019.xlsx', pathDb = './data/PSP_Data.sqlite', table = \"TB001_DATA_RAW\"):\n",
    "    \n",
    "    if not hf.checkIfTableDbExists(pathDb, table):\n",
    "        out = pd.read_excel(dataPath)\n",
    "        out = out.drop([\"Unnamed: 0\"], axis = 1)\n",
    "        out[\"tmsp\"] = pd.to_datetime(out[\"tmsp\"])\n",
    "        out = out.sort_values(by = [\"tmsp\"], ascending = True)\n",
    "\n",
    "        hf.writeDb(out, pathDb = pathDb, table_name = table)\n",
    "    else:\n",
    "        out = hf.readSqlTable(pathDb, table = table)\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ec0348-fa5e-43e6-8805-61d0d5e1ab82",
   "metadata": {},
   "source": [
    "As seen in the data understanding phase and also described by Mu (2021) and Mao, et al. (2023) also the success rate as a combination of the payent service provider, the card type and the 3D identification could be a valuable feature to integrate in the model. The time windows will be chosen wider in this case because the transaction density for those constellations is not very high. Mu (2021) proposed several days. In order to avoid spurious correlations only an overall expanding window calculation will be applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c81ca707-f101-4016-8693-3065ace52c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def applyDataCleaningFeatureEng(dataPath = './data/PSP_Jan_Feb_2019.xlsx', \n",
    "                                outPath = './data/data_prepared.csv', \n",
    "                                train_length = 0.7, \n",
    "                                pathDb = './data/PSP_Data.sqlite'\n",
    "                               ):\n",
    "    start_pipeline = time.time()\n",
    "    \n",
    "    if not hf.checkIfTableDbExists(pathDb, \"TB003_DATA_PREPARED\"):\n",
    "        start_time = time.time()\n",
    "        print('=== Start raw data loading ===')\n",
    "        out = get_raw_data(dataPath = dataPath, pathDb = pathDb)\n",
    "        print(\"=== Elapsed Time: \" + str(time.time() - start_time) + \" seconds ===\")\n",
    "        print(\"Shape of dataframe: \" + str(out.shape))\n",
    "\n",
    "        if not hf.checkIfTableDbExists(pathDb, \"TB002_DATA_CLEANED\"):\n",
    "            print(\"\")\n",
    "            start_time = time.time()\n",
    "            print('=== Start filter rows ===')\n",
    "            out = fr.selectRows(out)\n",
    "            print(\"=== Elapsed Time: \" + str(time.time() - start_time) + \" seconds ===\")\n",
    "            print(\"Shape of dataframe: \" + str(out.shape))\n",
    "\n",
    "            print(\"\")\n",
    "            start_time = time.time()\n",
    "            print('=== Start get timestamp information ===')\n",
    "            out = get_tmsp_information(out)\n",
    "            out = get_daytime(out)\n",
    "            print(\"=== Elapsed Time: \" + str(time.time() - start_time) + \" seconds ===\")\n",
    "            print(\"Shape of dataframe: \" + str(out.shape))\n",
    "\n",
    "            print(\"\")\n",
    "            start_time = time.time()\n",
    "            print('=== Start get amountgroups by Jenks natural breaks ===')\n",
    "            out = get_amountgroup(out)\n",
    "            hf.writeDb(out, pathDb = pathDb, table_name = \"TB002_DATA_CLEANED\")\n",
    "            print(\"=== Elapsed Time: \" + str(time.time() - start_time) + \" seconds ===\")\n",
    "            print(\"Shape of dataframe: \" + str(out.shape))\n",
    "        else:\n",
    "            print(\"=== Cleaned Data Table already exists - reading from DB ===\")\n",
    "            out = hf.readSqlTable(pathDb, \"TB002_DATA_CLEANED\")\n",
    "            print(\"Shape of dataframe: \" + str(out.shape))\n",
    "        \n",
    "        print(\"\")\n",
    "        start_time = time.time()\n",
    "        print(\"=== Start Feature Engineering ===\")\n",
    "        print(\"=== Get overall success rates ===\")\n",
    "        out = getOverallSR(out)\n",
    "        print(\"=== Get overall success rates for columns and column combinations\")\n",
    "        print(\"= PSP\")\n",
    "        out = combinatoric_SR(out, addColumns = [\"PSP\"])\n",
    "        print(\"= PSP x card\")\n",
    "        out = combinatoric_SR(out, addColumns = [\"PSP\", \"card\"])\n",
    "        print(\"= PSP x card x 3D_secured\")\n",
    "        out = combinatoric_SR(out, addColumns = [\"PSP\", \"card\", \"3D_secured\"])\n",
    "        print(\"= PSP x card x 3D_secured x amountgroup_word\")\n",
    "        out = combinatoric_SR(out, addColumns = [\"PSP\", \"card\", \"3D_secured\", \"amountgroup_word\"])\n",
    "        print(\"=== Elapsed Time: \" + str(time.time() - start_time) + \" seconds ===\")\n",
    "        print(\"Shape of dataframe: \" + str(out.shape))\n",
    "        \n",
    "        print(\"\")\n",
    "        start_time = time.time()\n",
    "        print(\"=== Get event window success rates for columns and column combinations\")\n",
    "        print(\"= PSP\")\n",
    "        out = combinatoric_event_window_SR(out, addColumns = [\"PSP\"])\n",
    "        print(\"= PSP x card\")\n",
    "        out = combinatoric_event_window_SR(out, addColumns = [\"PSP\", \"card\"])\n",
    "        print(\"= PSP x card x 3D_secured\")\n",
    "        out = combinatoric_event_window_SR(out, addColumns = [\"PSP\", \"card\", \"3D_secured\"])\n",
    "        print(\"= PSP x card x 3D_secured x amountgroup_word\")\n",
    "        out = combinatoric_event_window_SR(out, addColumns = [\"PSP\", \"card\", \"3D_secured\", \"amountgroup_word\"])\n",
    "        print(\"=== Elapsed Time: \" + str(time.time() - start_time) + \" seconds ===\")\n",
    "        print(\"Shape of dataframe: \" + str(out.shape))\n",
    "        \n",
    "        print(\"\")\n",
    "        start_time = time.time()\n",
    "        print(\"=== Get time window success rates for columns and column combinations\")\n",
    "        print(\"= PSP\")\n",
    "        out = combinatoric_time_window_SR(out, addColumns = [\"PSP\"])\n",
    "        print(\"= PSP x card\")\n",
    "        out = combinatoric_time_window_SR(out, addColumns = [\"PSP\", \"card\"])\n",
    "        print(\"= PSP x card x 3D_secured\")\n",
    "        out = combinatoric_time_window_SR(out, addColumns = [\"PSP\", \"card\", \"3D_secured\"])\n",
    "        print(\"= PSP x card x 3D_secured x amountgroup_word\")\n",
    "        out = combinatoric_time_window_SR(out, addColumns = [\"PSP\", \"card\", \"3D_secured\", \"amountgroup_word\"])\n",
    "        print(\"=== Elapsed Time: \" + str(time.time() - start_time) + \" seconds ===\")\n",
    "        print(\"Shape of dataframe: \" + str(out.shape))\n",
    "        \n",
    "        hf.writeDb(out, pathDb = pathDb, table_name = \"TB003_DATA_PREPARED\")\n",
    "    \n",
    "    else:\n",
    "        print(\"=== Prepared Data already exists - reading from DB ===\")\n",
    "        out = hf.readSqlTable(pathDb, \"TB003_DATA_PREPARED\")\n",
    "     \n",
    "    print(\"\")\n",
    "    print(\"============================\")\n",
    "    print(\"= time for whole pipeline: \" + str(time.time() - start_pipeline) + \" seconds\")\n",
    "    print(\"============================\")\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e420f871-1b4c-4dbf-8134-785b87f5b19b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Table already exists ===\n",
      "=== Start raw data loading ===\n",
      "=== Table already exists ===\n",
      "=== Table TB001_DATA_RAW created successful ===\n",
      "=== Elapsed Time: 6.3144142627716064 seconds ===\n",
      "Shape of dataframe: (50410, 7)\n",
      "=== Table already exists ===\n",
      "\n",
      "=== Start filter rows ===\n",
      "= Half time: 171.1493136882782 seconds\n",
      "= End Time: 300.4188566207886 seconds\n",
      "=== Elapsed Time: 300.4188566207886 seconds ===\n",
      "Shape of dataframe: (27491, 12)\n",
      "\n",
      "=== Start get timestamp information ===\n",
      "=== Elapsed Time: 0.38375306129455566 seconds ===\n",
      "Shape of dataframe: (27491, 20)\n",
      "\n",
      "=== Start get amountgroups by Jenks natural breaks ===\n",
      "= Jenks natural breaks are:\n",
      "[0, 99, 175, 247, 330, 10000]\n",
      "=== Table TB002_DATA_CLEANED created successful ===\n",
      "=== Elapsed Time: 1.2694461345672607 seconds ===\n",
      "Shape of dataframe: (27491, 21)\n",
      "\n",
      "=== Start Feature Engineering ===\n",
      "=== Get overall success rates ===\n",
      "=== Get overall success rates for columns and column combinations\n",
      "= PSP\n",
      "PSP_SR\n",
      "= PSP x card\n",
      "PSP_card_SR\n",
      "= PSP x card x 3D_secured\n",
      "PSP_card_3D_secured_SR\n",
      "= PSP x card x 3D_secured x amountgroup_word\n",
      "PSP_card_3D_secured_amountgroup_word_SR\n",
      "=== Elapsed Time: 1.3748078346252441 seconds ===\n",
      "Shape of dataframe: (27491, 26)\n",
      "\n",
      "=== Get event window success rates for columns and column combinations\n",
      "= PSP\n",
      "= Event window size: 5\n",
      "PSP_e5_SR\n",
      "= Event window size: 10\n",
      "PSP_e10_SR\n",
      "= Event window size: 100\n",
      "PSP_e100_SR\n",
      "= Event window size: 200\n",
      "PSP_e200_SR\n",
      "= PSP x card\n",
      "= Event window size: 5\n",
      "PSP_card_e5_SR\n",
      "= Event window size: 10\n",
      "PSP_card_e10_SR\n",
      "= Event window size: 100\n",
      "PSP_card_e100_SR\n",
      "= Event window size: 200\n",
      "PSP_card_e200_SR\n",
      "= PSP x card x 3D_secured\n",
      "= Event window size: 5\n",
      "PSP_card_3D_secured_e5_SR\n",
      "= Event window size: 10\n",
      "PSP_card_3D_secured_e10_SR\n",
      "= Event window size: 100\n",
      "PSP_card_3D_secured_e100_SR\n",
      "= Event window size: 200\n",
      "PSP_card_3D_secured_e200_SR\n",
      "= PSP x card x 3D_secured x amountgroup_word\n",
      "= Event window size: 5\n",
      "PSP_card_3D_secured_amountgroup_word_e5_SR\n",
      "= Event window size: 10\n",
      "PSP_card_3D_secured_amountgroup_word_e10_SR\n",
      "= Event window size: 100\n",
      "PSP_card_3D_secured_amountgroup_word_e100_SR\n",
      "= Event window size: 200\n",
      "PSP_card_3D_secured_amountgroup_word_e200_SR\n",
      "--- Number of missing values too large ---\n",
      "=== Elapsed Time: 10.453545093536377 seconds ===\n",
      "Shape of dataframe: (27491, 41)\n",
      "\n",
      "=== Get time window success rates for columns and column combinations\n",
      "= PSP\n",
      "= Time window size: 1h\n",
      "PSP_t1h_SR\n",
      "= Time window size: 6h\n",
      "PSP_t6h_SR\n",
      "= Time window size: 12h\n",
      "PSP_t12h_SR\n",
      "= Time window size: 24h\n",
      "PSP_t24h_SR\n",
      "= Time window size: 72h\n",
      "PSP_t72h_SR\n",
      "= PSP x card\n",
      "= Time window size: 1h\n",
      "PSP_card_t1h_SR\n",
      "--- Number of missing values too large: 0.1622712887854207 ---\n",
      "= Time window size: 6h\n",
      "PSP_card_t6h_SR\n",
      "= Time window size: 12h\n",
      "PSP_card_t12h_SR\n",
      "= Time window size: 24h\n",
      "PSP_card_t24h_SR\n",
      "= Time window size: 72h\n",
      "PSP_card_t72h_SR\n",
      "= PSP x card x 3D_secured\n",
      "= Time window size: 1h\n",
      "PSP_card_3D_secured_t1h_SR\n",
      "--- Number of missing values too large: 0.30551816958277256 ---\n",
      "= Time window size: 6h\n",
      "PSP_card_3D_secured_t6h_SR\n",
      "= Time window size: 12h\n",
      "PSP_card_3D_secured_t12h_SR\n",
      "= Time window size: 24h\n",
      "PSP_card_3D_secured_t24h_SR\n",
      "= Time window size: 72h\n",
      "PSP_card_3D_secured_t72h_SR\n",
      "= PSP x card x 3D_secured x amountgroup_word\n",
      "= Time window size: 1h\n",
      "PSP_card_3D_secured_amountgroup_word_t1h_SR\n",
      "--- Number of missing values too large: 0.7099050598377651 ---\n",
      "= Time window size: 6h\n",
      "PSP_card_3D_secured_amountgroup_word_t6h_SR\n",
      "--- Number of missing values too large: 0.2568476956094722 ---\n",
      "= Time window size: 12h\n",
      "PSP_card_3D_secured_amountgroup_word_t12h_SR\n",
      "= Time window size: 24h\n",
      "PSP_card_3D_secured_amountgroup_word_t24h_SR\n",
      "= Time window size: 72h\n",
      "PSP_card_3D_secured_amountgroup_word_t72h_SR\n",
      "=== Elapsed Time: 49.957651138305664 seconds ===\n",
      "Shape of dataframe: (27491, 57)\n",
      "=== Table TB003_DATA_PREPARED created successful ===\n",
      "\n",
      "============================\n",
      "= time for whole pipeline: 370.66952681541443 seconds\n",
      "============================\n"
     ]
    }
   ],
   "source": [
    "data_clean = applyDataCleaningFeatureEng()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb91387-0ff9-49b7-a52e-2493545c4ec4",
   "metadata": {},
   "source": [
    "The column ```tmsp``` is included in the additionally created columns ```month```, ```dayOfMonth```, ```weekday```, ```holiday```, ```daytime``` and ```minuteOfDay```, so this column can be deleted. Furthermore the column cannot be used in any machine learning model.\n",
    "\n",
    "The columns ```daytime``` and ```time``` were created for data exploration reasons only. The columns can be completely reproduced by the column ```minuteOfDay```. So also the columns ```daytime``` and ```time``` can also be deleted.\n",
    "\n",
    "Also the columns ```amountgroup``` and ```amountgroup_word``` can be completely derived from the column ```amount``` and are artifacts from the previous data understanding steps. Highly correlated features containing redundant information can cause problems in many ML settings, so both columns will be removed.\n",
    "\n",
    "Also the feature ```failPrevious``` which is a dummy-variable to indicate if a transaction has failed previously or not.\n",
    "\n",
    "The columns ```lower```, ```upper```, ```numLower``` and ```numUpper``` were created for row-selection reasons and can also be excluded as features for the modeling phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "89769855-02c0-4254-af63-21366bf08eb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.41679289104311823\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    print(data_clean[data_clean['PSP'] != \"Simplecard\"].success.mean())\n",
    "    data_clean_dropped_woTime = hf.dropColumns(data = data_clean.copy(), \n",
    "        columns = ['tmsp_hour', 'daytime', 'time', 'failedPSP', 'amountgroup_word', 'lower', 'upper', 'numUpper'])\n",
    "except:\n",
    "    print(\"=== Object does not exist ===\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae2c7c0-187d-4766-bbfe-77acf5cc085b",
   "metadata": {},
   "source": [
    "## Format Data\n",
    "**Reformatted Data** <br>\n",
    "As a final step before model building, it is helpful to check whether certain techniques require a particular format or order to the data. For example, it is not uncommon that a sequence algorithm requires the data to be presorted before running the model. Even if the model can perform the sorting for you, it may save processing time to use a Sort node prior to modeling. Task List Consider the following questions when formatting data: \n",
    "* Which models do you plan to use? \n",
    "* Do these models require a particular data format or order? If changes are recommended, the processing tools in IBM SPSS Modeler can help you apply the necessary data manipulation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e5fcca-6a37-47aa-9282-e595a661ca17",
   "metadata": {},
   "source": [
    "Treebased models are particularly useful in dealing with categorical features and finding insightful breakpoints in continuous variables. From a modeling perspective it seems reasonable to achieve good modeling results with treebased models. Furthermore the dataset is not very large and contains only structured data. From a performance perspective ```XGBoost``` models have proven to be particularly competitive in Kaggle competitions for the aforementioned data settings. Because ```XGBoost``` models are harder to tune and tend to overfit also ```Random Forest``` models will be considered in a first modeling iteration. So ```Random Forest``` will be the starting point when it comes to the feature selection phase. In order to do so, the data has to be formatted in such a way, that an ```XGBoost``` implementation in Python can deal with the dataset. This means all categorical and ordinal features in the dataset have to be enconded to numerical features. This can be achieved by One-Hot or Label-Encoding.\n",
    "\n",
    "One-Hot encoding is useful, when the categorical variables do not have too many unique values. Label-Encoding can be useful, when the categorical variable have many unique values. Label-Encoding is also used for ordinal variables. So the cardinality of the categorical variables in the dataset has to be inspected first. The categorical features in the dataset are:\n",
    "* country\n",
    "* success\n",
    "* PSP\n",
    "* 3D_secured\n",
    "* card\n",
    "* month\n",
    "* dayOfMonth\n",
    "* weekday\n",
    "* weekend\n",
    "* holiday\n",
    "\n",
    "From those variables the following variables are already in a numeric format and can be used in ML models:\n",
    "* success\n",
    "* 3D_secured\n",
    "* dayOfMonth\n",
    "* weekend\n",
    "* holiday\n",
    "\n",
    "The cardinality of the remaining variables are:\n",
    "* PSP: 4\n",
    "* card: 3\n",
    "* month: 2\n",
    "* weekday: 7\n",
    "\n",
    "This shows that the yet to be formatted variables have a low cardinality and will be One-Hot-transformed to become useful in terms of modeling purpose. In order to have a stable and robust data setup for the subsequent step and some models assume that one dummy category is left out because the value can be perfectly derived from the other dummy variables, as well as that for some modeling approaches like linear models continuous data should be normalized, we also do that in terms of data formatting. This step is done after splitting data into train and test data.\n",
    "\n",
    "In terms of the remaining continuous variable ```amount``` we don´t have to normalize the data because we are preparing for a treebased modeling szenario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e735b8df-3f11-4ac9-854a-2a6f530b199e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatData(data, columns = ['PSP', 'card', 'month', 'weekday', 'country']):\n",
    "    out = data.copy()\n",
    "    \n",
    "    out = pd.get_dummies(out, columns=columns, drop_first=True)\n",
    "    print(\"=== Number of missing values ===\")\n",
    "    print(out.isna().sum().sum())\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e58c8329-4556-4a8b-9257-b3fab93ea6ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Number of missing values ===\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "data_formatted_time = formatData(data_clean_dropped_woTime)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2407b966-85ce-417c-b46d-9e6606f1ea32",
   "metadata": {},
   "source": [
    "In order to prepare the subsequent modeling steps we first have to split the data into a train-validate-test design. For the problem at hand, we can do that by a applying a time-based splitting strategy or a random splitting strategy. A well established splitting ratio in many ML settings is 70 % train, 15 % validate and 15 % test data. Because we have an imbalanced data set, it is better to choose a stratified splitting strategy in order to have the same ratios of success and no-success observations in the datasets for the random splitting approach. Because we do not know at this time, which model performs best for the given task, all numerical variables are brought to a unified scale using a min-max scaling approach. This approach brings a numerical feature into a specified range. Because there are many dummy variables in the dataset, the range was fixed between 0 and one.\n",
    "\n",
    "***\n",
    "Min-Max Scaler<br>\n",
    "***\n",
    "$$x_{scaled} = \\displaystyle \\frac{x - x_{min}}{x_{max} - x_{min}}$$\n",
    "***\n",
    "\n",
    "As a first step we have to separate the X-matrix with all intended features and the y-vector. In this case the y-vector is the column ```success```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5eeb97f0-6e3a-4c9b-8493-fb96d1af41a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getColumnsToScale(data):\n",
    "    from pandas.api.types import is_numeric_dtype\n",
    "    out = []\n",
    "    for column in data.copy().columns:\n",
    "        if is_numeric_dtype(data.copy()[column]):\n",
    "            if data.copy()[column].max() > 1:\n",
    "                out.append(column)\n",
    "        else:\n",
    "            print(\"Column \" + column + \" is not numeric\")\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6aec9398-badd-407b-85d1-66a369c577aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def applyRandomSplitting(data, train_size = 0.7, test_size = 0.15, validate_size = 0.15):\n",
    "    applyData = data.copy()\n",
    "    y = applyData['success']\n",
    "    X = hf.dropColumns(applyData, columns = [\"success\"])\n",
    "    \n",
    "    scale_columns = getColumnsToScale(applyData)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=test_size + validate_size, random_state=1977)\n",
    "    X_test, X_validate, y_test, y_validate = train_test_split(X_test, y_test, stratify=y_test, \n",
    "                                                              test_size=test_size/(test_size + validate_size), \n",
    "                                                              random_state=1977)\n",
    "    scaler = MinMaxScaler()\n",
    "    X_train[scale_columns] = scaler.fit_transform(X_train[scale_columns])\n",
    "    X_test[scale_columns] = scaler.transform(X_test[scale_columns])\n",
    "    X_validate[scale_columns] = scaler.transform(X_validate[scale_columns])\n",
    "    \n",
    "    print(\"= Success rate in y_train: \" + str(y_train.sum()/len(y_train)))\n",
    "    print(\"= Success rate in y_validate: \" + str(y_validate.mean()))\n",
    "    print(\"= Success rate in y_test: \" + str(y_test.mean()))\n",
    "    \n",
    "    return (X, y, X_train, y_train, X_validate, y_validate, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0986b296-d18c-4fad-a8a2-5f00f49dc593",
   "metadata": {},
   "outputs": [],
   "source": [
    "def applyTimeSplitting(data, train_size = 0.7, test_size = 0.15, validate_size = 0.15, time_col = \"tmsp\"):\n",
    "    applyData = data.copy()\n",
    "    applyData = applyData.sort_values(by = [time_col], ascending = True)\n",
    "    length = len(applyData)\n",
    "    \n",
    "    train_length = int(np.round(length*train_size))\n",
    "    test_length = int(length - train_length)\n",
    "    validate_length = int(np.round(test_length * (validate_size/(validate_size + test_size))))\n",
    "    test_length = int(test_length - validate_length)\n",
    "    \n",
    "    y = applyData['success']\n",
    "    X = hf.dropColumns(applyData, columns = [\"success\", time_col])\n",
    "    \n",
    "    assert (test_length + validate_length + train_length) == length, f\"number expected: {length}, got: {test_length + validate_length + train_length}\"\n",
    "    \n",
    "    X_train = X.copy().iloc[:train_length, :]\n",
    "    y_train = y.copy().iloc[:train_length]\n",
    "    X_validate = X.copy().iloc[train_length:(train_length + validate_length), :]\n",
    "    y_validate = y.copy().iloc[train_length:(train_length + validate_length)]\n",
    "    X_test = X.copy().iloc[(train_length + validate_length):, :]\n",
    "    y_test = y.copy().iloc[(train_length + validate_length):]\n",
    "    \n",
    "    assert (len(X_train) + len(X_validate) + len(X_test)) == length, f\"number expected: {length}, got: {(len(X_train) + len(X_validate) + len(X_test))}\"\n",
    "    \n",
    "    scale_columns = getColumnsToScale(X)\n",
    "    scaler = MinMaxScaler()\n",
    "    X_train[scale_columns] = scaler.fit_transform(X_train[scale_columns])\n",
    "    parameters = hf.loadPickle('./data/parameters.pkl')\n",
    "    parameters[\"scaler\"] = scaler\n",
    "    parameters[\"scale_columns\"] = scale_columns\n",
    "    hf.writePickle('./data/parameters.pkl', parameters)\n",
    "    X_test[scale_columns] = scaler.transform(X_test[scale_columns])\n",
    "    X_validate[scale_columns] = scaler.transform(X_validate[scale_columns])\n",
    "    \n",
    "    print(\"= Success rate in y_train: \" + str(y_train.mean()))\n",
    "    print(\"= Success rate in y_validate: \" + str(y_validate.mean()))\n",
    "    print(\"= Success rate in y_test: \" + str(y_test.mean()))\n",
    "    \n",
    "    return (X, y, X_train, y_train, X_validate, y_validate, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9955829e-ec67-4520-9140-bda31dd95a3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "= Success rate in y_train: 0.3805861567241738\n",
      "= Success rate in y_validate: 0.34990300678952474\n",
      "= Success rate in y_test: 0.352413291292748\n",
      "=== Table already exists ===\n",
      "=== Table X created successful ===\n",
      "=== Table y created successful ===\n",
      "=== Table already exists ===\n",
      "=== Table X_train created successful ===\n",
      "=== Table y_train created successful ===\n",
      "=== Table already exists ===\n",
      "=== Table X_validate created successful ===\n",
      "=== Table y_validate created successful ===\n",
      "=== Table already exists ===\n",
      "=== Table X_test created successful ===\n",
      "=== Table y_test created successful ===\n"
     ]
    }
   ],
   "source": [
    "X, y, X_train, y_train, X_validate, y_validate, X_test, y_test = applyTimeSplitting(data_formatted_time)\n",
    "if not hf.checkIfTableDbExists('./data/PSP_Data.sqlite', \"X\"):\n",
    "    hf.writeDb(X, pathDb = './data/PSP_Data.sqlite', table_name = \"X\")\n",
    "    hf.writeDb(y, pathDb = './data/PSP_Data.sqlite', table_name = \"y\")\n",
    "if not hf.checkIfTableDbExists('./data/PSP_Data.sqlite', \"X_train\"):\n",
    "    hf.writeDb(X_train, pathDb = './data/PSP_Data.sqlite', table_name = \"X_train\")\n",
    "    hf.writeDb(y_train, pathDb = './data/PSP_Data.sqlite', table_name = \"y_train\")\n",
    "if not hf.checkIfTableDbExists('./data/PSP_Data.sqlite', \"X_validate\"):\n",
    "    hf.writeDb(X_validate, pathDb = './data/PSP_Data.sqlite', table_name = \"X_validate\")\n",
    "    hf.writeDb(y_validate, pathDb = './data/PSP_Data.sqlite', table_name = \"y_validate\")\n",
    "if not hf.checkIfTableDbExists('./data/PSP_Data.sqlite', \"X_test\"):\n",
    "    hf.writeDb(X_test, pathDb = './data/PSP_Data.sqlite', table_name = \"X_test\")\n",
    "    hf.writeDb(y_test, pathDb = './data/PSP_Data.sqlite', table_name = \"y_test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6e8a10-22e9-4cf0-8c61-e1bf46ba4011",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3469c20c-9eac-4cc5-a06e-605a13b3001b",
   "metadata": {},
   "source": [
    "<p>Arlot, S., & Celisse, A. (2010). A survey of cross-validation procedures for model selection. Statistics Surveys, 4(none). https://doi.org/10.1214/09-SS054</p>\n",
    "<p>Athanasopoulos, G., & Hyndman, R. J. (2021). Forecasting: Principles and Practice (3. Aufl.). OTexts. OTexts.com/fpp3</p>\n",
    "<p>Bygari, R., Gupta, A., Raghuvanshi, S., Bapna, A., & Sahu, B. (2021). An AI-powered Smart Routing Solution for Payment Systems. 2026–2033. https://doi.org/10.1109/BigData52589.2021.9671961</p>\n",
    "<p>Chetcuti, J. (2020). PhiCor: Calculating Phi coefficient of Association. (edsbas.C61D16BC). BASE. https://doi.org/10.5281/zenodo.3898308\n",
    "IBM Corporation. (2021). IBM Documentation: IBM SPSS Modeler CRISP-DM Guide. https://www.ibm.com/docs/en/spss-modeler/18.1.1?topic=spss-modeler-crisp-dm-guide</p>\n",
    "<p>Kornbrot, D. (2005). Point Biserial Correlation. https://doi.org/10.1002/0470013192.bsa485</p>\n",
    "<p>Kuhn, M., & Johnson, K. (2013). Applied Predictive Modeling (1st ed. 2013, Corr. 2nd printing 2018 Edition). Springer.</p>\n",
    "<p>Kuhn, M., & Johnson, K. (2019). Feature Engineering and Selection: A Practical Approach for Predictive Models. Taylor & Francis Ltd. http://www.feat.engineering/</p>\n",
    "<p>Lakshmanan, V., Robinson, S., & Munn, M. (2020). Machine Learning Design Patterns: Solutions to Common Challenges in Data Preparation, Model Building, and MLOps. O’Reilly Media.</p>\n",
    "<p>Leonard, M., & Wolfe, B. (2005). Mining transactional and time series data. abstract, presentation and paper, SUGI, 10–13.</p>\n",
    "<p>Mao, X., Xu, S., Kumar, R., R, V., Hong, X., & Menghani, D. (2023). Improving the customer’s experience via ML-driven payment routing. https://engineering.linkedin.com/blog/2023/improving-the-customer-s-experience-via-ml-driven-payment-routin</p>\n",
    "<p>Mu, L. (2021). Using Machine Learning to Improve Payment Authorization Rate | The PayPal Technology Blog. https://medium.com/paypal-tech/using-machine-learning-to-improve-payment-authorization-rates-bc3b2cbf4999</p>\n",
    "<p>Wirth, R., & Hipp, J. (2000). CRISP-DM: Towards a standard process model for data mining. Proceedings of the 4th International Conference on the Practical Applications of Knowledge Discovery and Data Mining.</p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
