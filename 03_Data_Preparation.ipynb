{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab95ffc0-5989-4435-b12d-8481711c61e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jenkspy import JenksNaturalBreaks\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import seaborn as sns\n",
    "import itertools\n",
    "import sqlite3\n",
    "\n",
    "import modules.feature_selection as fs\n",
    "import modules.helper_functions as hf\n",
    "import modules.filter_rows as fr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a8abae-bdbe-451f-ab9b-a524775a4d5f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c903743d-f1af-478b-b391-54dd436d01f2",
   "metadata": {},
   "source": [
    "All helper functions are included in the module ```helper_functions.py```, such that they can also be used in other notebooks as well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0a8935-84e8-4720-b36b-257916d4960e",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa83731e-a3c7-4a15-b5fb-af153f10ee72",
   "metadata": {},
   "source": [
    "## Feature Engineering and Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "effe04ae-dc65-45ae-af85-d406e4645ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tmsp_information(data):\n",
    "    out = data.copy()\n",
    "    out[\"month\"] = out.copy()[\"tmsp\"].dt.strftime('%b')\n",
    "    out[\"dayOfMonth\"] = out.copy()[\"tmsp\"].dt.strftime('%#d').astype(int)\n",
    "    out[\"weekday\"] = out.copy().tmsp.dt.day_name()\n",
    "    out[\"weekend\"] = np.where(out['weekday'].isin(['Saturday', 'Sunday']), 1, 0)\n",
    "    out[\"holiday\"] = np.where((out['month'] == 'Jan') & (out['dayOfMonth'] == 1), 1, 0)\n",
    "    \n",
    "    return out\n",
    "\n",
    "def get_daytime(data):\n",
    "    out = data.copy()\n",
    "    \n",
    "    out['time'] = out['tmsp'].dt.strftime('%H:%M')\n",
    "    out['daytime'] = np.where((out['time'] >= '00:00') & (out['time'] < '06:00'), 'night', \n",
    "                        np.where((out['time'] >= '06:00') & (out['time'] < '12:00'), 'morning',\n",
    "                        np.where((out['time'] >= '12:00') & (out['time'] < '18:00'), 'afternoon', 'evening')))\n",
    "    out[\"minuteOfDay\"] = (out[\"tmsp\"].dt.hour * 60) + (out[\"tmsp\"].dt.minute)\n",
    "    \n",
    "    return out\n",
    "\n",
    "def get_amountgroup(data, train_length = 0.7, on_training = True):\n",
    "    out = data.copy()\n",
    "    out = out.sort_values(by = [\"tmsp\"], ascending = True)\n",
    "    out_length = len(out)\n",
    "    parameters = {}\n",
    "    \n",
    "    if on_training:\n",
    "        train_split = int(np.round(out_length*train_length))\n",
    "        parameters['train_split_iloc'] = train_split\n",
    "    else:\n",
    "        train_split = out_length\n",
    "    \n",
    "    amount = list(out.iloc[:train_split,:]['amount'])\n",
    "    \n",
    "    jnb = JenksNaturalBreaks(5)\n",
    "    jnb.fit(amount)\n",
    "    bins = jnb.breaks_\n",
    "    bins[0] = 0\n",
    "    bins[len(bins) - 1] = 10000\n",
    "    parameters[\"jenks\"] = bins\n",
    "    hf.writePickle('./data/parameters.pkl', parameters)\n",
    "    \n",
    "    out = hf.get_amountgroups(out, bins = bins)\n",
    "    \n",
    "    print(\"= Jenks natural breaks are:\")\n",
    "    print(jnb.breaks_)\n",
    "    \n",
    "    return out\n",
    "\n",
    "def getCard_3DSec_PSP_Amountgroup(data):\n",
    "    y = hf.get_y(groups=[\"card\", \"3D_secured\", \"PSP\", \"amountgroup_word\"], data = data.copy())\n",
    "    y = y.sort_values(by=['success_rate'], ascending = False)\n",
    "    y_mod = y.drop('PSP', axis=1)\n",
    "    y_mod = y_mod.groupby([\"card\", \"3D_secured\", \"amountgroup_word\"]).aggregate({'success_rate': 'max'}).reset_index()\n",
    "    y_mod = y_mod.sort_values(by=[\"success_rate\"], ascending = False)\n",
    "    y_mod = y_mod.merge(y, how=\"left\", on = [\"card\", \"3D_secured\", \"amountgroup_word\", \"success_rate\"])\n",
    "\n",
    "    return y_mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "298dbb24-e86a-4927-819e-f5720f28f493",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getOverallSR(data, train_split = 0.7):\n",
    "    out = data.copy()\n",
    "    train_split = int(np.round(len(out)*train_split))\n",
    "    out[\"overallSR\"] = out.iloc[:train_split, :].success.mean()\n",
    "                \n",
    "    return out\n",
    "\n",
    "def combinatoric_SR(data, addColumns = [\"PSP\", \"card\", \"3D_secured\", \"amountgroup_word\"], train_split = 0.7):\n",
    "    out = data.copy()\n",
    "\n",
    "    train_split = int(np.round(len(out)*train_split))\n",
    "    combinations = {}\n",
    "    colName = \"\"\n",
    "    for col in addColumns:\n",
    "        combinations[col] = list(out[col].unique())\n",
    "        colName = colName + col + \"_\"\n",
    "\n",
    "    colName = colName + \"SR\"\n",
    "    print(colName)\n",
    "    addColumns.append(colName)\n",
    "\n",
    "    keys, values = zip(*combinations.items())\n",
    "    permutations_dicts = [dict(zip(keys, v)) for v in itertools.product(*values)]\n",
    "\n",
    "    joinFrame = pd.DataFrame()\n",
    "\n",
    "    i = 1\n",
    "    for permutation in permutations_dicts:\n",
    "        subset = out.copy().iloc[:train_split, :]\n",
    "        for key in permutation.keys():\n",
    "            subset = subset[subset[key] == permutation[key]]\n",
    "        subset[colName] = subset.success.mean()\n",
    "        joinFrame = pd.concat([joinFrame, subset[addColumns]])\n",
    "\n",
    "    out = out.merge(joinFrame.drop_duplicates(), how = 'left', on = list(set(out.columns).intersection(set(joinFrame.columns))))\n",
    "    \n",
    "    return out\n",
    "\n",
    "def combinatoric_event_window_SR(data, \n",
    "                                 addColumns = [\"PSP\", \"card\", \"3D_secured\", \"amountgroup_word\"], \n",
    "                                 event_windows = [5, 10, 100, 200],\n",
    "                                 allowed_missing = 0.05\n",
    "                                ):\n",
    "    out = data.copy()\n",
    "\n",
    "    for event_window in event_windows:\n",
    "        print(\"= Event window size: \" + str(event_window))\n",
    "        combinations = {}\n",
    "        colName = \"\"\n",
    "        replaceCol = \"\"\n",
    "        for col in addColumns:\n",
    "            combinations[col] = list(out[col].unique())\n",
    "            colName = colName + col + \"_\"\n",
    "            replaceCol = replaceCol + col + \"_\"\n",
    "\n",
    "        colName = colName + \"e\" + str(event_window) + \"_SR\"\n",
    "        replaceCol = replaceCol + \"SR\"\n",
    "        print(colName)\n",
    "        outCols = addColumns.copy()\n",
    "        outCols.append(colName)\n",
    "\n",
    "        keys, values = zip(*combinations.items())\n",
    "        permutations_dicts = [dict(zip(keys, v)) for v in itertools.product(*values)]\n",
    "\n",
    "        joinFrame = pd.DataFrame()\n",
    "\n",
    "        for permutation in permutations_dicts:\n",
    "            subset = out.copy()\n",
    "            for key in permutation.keys():\n",
    "                subset = subset[subset[key] == permutation[key]]\n",
    "            subset[colName] = subset.success.shift().rolling(\n",
    "                event_window, min_periods=int(np.ceil(event_window/10))\n",
    "            ).mean()\n",
    "            joinFrame = pd.concat([joinFrame, subset[outCols]])\n",
    "        \n",
    "        missing_ratio = joinFrame.isna().sum().sum()/len(joinFrame)\n",
    "        if missing_ratio <= allowed_missing:\n",
    "            out = out.join(joinFrame[colName])\n",
    "            out[colName] = out[colName].fillna(out[replaceCol])\n",
    "        else:\n",
    "            print(\"--- Number of missing values too large ---\")\n",
    "    \n",
    "    return out\n",
    "\n",
    "def combinatoric_time_window_SR(data,\n",
    "                                addColumns = [\"PSP\", \"card\", \"3D_secured\", \"amountgroup_word\"], \n",
    "                                time_windows = [1, 6, 12, 24, 72],\n",
    "                                allowed_missing = 0.15\n",
    "                                ):\n",
    "    out = data.copy()\n",
    "\n",
    "    for time_window in time_windows:\n",
    "        print(\"= Time window size: \" + str(time_window) + \"h\")\n",
    "        combinations = {}\n",
    "        colName = \"\"\n",
    "        replaceCol = \"\"\n",
    "        for col in addColumns:\n",
    "            combinations[col] = list(out[col].unique())\n",
    "            colName = colName + col + \"_\"\n",
    "            replaceCol = replaceCol + col + \"_\"\n",
    "\n",
    "        colName = colName + \"t\" + str(time_window) + \"h_SR\"\n",
    "        replaceCol = replaceCol + \"SR\"\n",
    "        print(colName)\n",
    "        outCols = addColumns.copy()\n",
    "        outCols.append(colName)\n",
    "\n",
    "        keys, values = zip(*combinations.items())\n",
    "        permutations_dicts = [dict(zip(keys, v)) for v in itertools.product(*values)]\n",
    "\n",
    "        joinFrame = pd.DataFrame()\n",
    "\n",
    "        for permutation in permutations_dicts:\n",
    "            subset = out.copy()\n",
    "            for key in permutation.keys():\n",
    "                subset = subset[subset[key] == permutation[key]]\n",
    "            subset[colName] = subset[[\"tmsp\", \"success\"]].rolling(\n",
    "                            str(time_window) + \"h\", on = \"tmsp\", min_periods=int(np.min([np.round(time_window/10), 3]))\n",
    "                        ).apply(hf.getMeanRollingEvent)[\"success\"]\n",
    "            joinFrame = pd.concat([joinFrame, subset[outCols]])\n",
    "\n",
    "        missing_ratio = joinFrame.isna().sum().sum()/len(joinFrame)\n",
    "        if missing_ratio <= allowed_missing:\n",
    "            out = out.join(joinFrame[colName])\n",
    "            out[colName] = out[colName].fillna(out[replaceCol])\n",
    "        else:\n",
    "            print(\"--- Number of missing values too large: \" + str(missing_ratio) + \" ---\")\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b427336c-4c12-4fd3-97ad-31796f12bb30",
   "metadata": {},
   "source": [
    "Bygari, et al. (2021) developed a similar routing approach for an India-based payment service provider called Razorpay. Instead of different payment service providers the usecase has several terminals. For the described \"Smart Routing Solution\" the authors also proposed to calculate the success rates based on different event- and time-windows. This means that the success rates for a transaction are also calculated based on a rolling-window approach.\n",
    "\n",
    "Furthermore all window-based feature engineering approaches can be computed overall and based on different features like ```PSP```, ```country``` or ```card```, etc.. This will be limited to the column ```PSP``` for this case-study."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f29d4fd2-76c4-4187-88d8-b442751038a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_raw_data(dataPath = './data/PSP_Jan_Feb_2019.xlsx', pathDb = './data/PSP_Data.sqlite', table = \"TB001_DATA_RAW\"):\n",
    "    \n",
    "    if not hf.checkIfTableDbExists(pathDb, table):\n",
    "        out = pd.read_excel(dataPath)\n",
    "        out = out.drop([\"Unnamed: 0\"], axis = 1)\n",
    "        out[\"tmsp\"] = pd.to_datetime(out[\"tmsp\"])\n",
    "        out = out.sort_values(by = [\"tmsp\"], ascending = True)\n",
    "\n",
    "        hf.writeDb(out, pathDb = pathDb, table_name = table)\n",
    "    else:\n",
    "        out = hf.readSqlTable(pathDb, table = table)\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c81ca707-f101-4016-8693-3065ace52c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def applyDataCleaningFeatureEng(dataPath = './data/PSP_Jan_Feb_2019.xlsx', \n",
    "                                outPath = './data/data_prepared.csv', \n",
    "                                train_length = 0.7, \n",
    "                                pathDb = './data/PSP_Data.sqlite'\n",
    "                               ):\n",
    "    start_pipeline = time.time()\n",
    "    \n",
    "    if not hf.checkIfTableDbExists(pathDb, \"TB003_DATA_PREPARED\"):\n",
    "        start_time = time.time()\n",
    "        print('=== Start raw data loading ===')\n",
    "        out = get_raw_data(dataPath = dataPath, pathDb = pathDb)\n",
    "        print(\"=== Elapsed Time: \" + str(time.time() - start_time) + \" seconds ===\")\n",
    "        print(\"Shape of dataframe: \" + str(out.shape))\n",
    "\n",
    "        if not hf.checkIfTableDbExists(pathDb, \"TB002_DATA_CLEANED\"):\n",
    "            print(\"\")\n",
    "            start_time = time.time()\n",
    "            print('=== Start filter rows ===')\n",
    "            out = fr.selectRows(out)\n",
    "            print(\"=== Elapsed Time: \" + str(time.time() - start_time) + \" seconds ===\")\n",
    "            print(\"Shape of dataframe: \" + str(out.shape))\n",
    "\n",
    "            print(\"\")\n",
    "            start_time = time.time()\n",
    "            print('=== Start get timestamp information ===')\n",
    "            out = get_tmsp_information(out)\n",
    "            out = get_daytime(out)\n",
    "            print(\"=== Elapsed Time: \" + str(time.time() - start_time) + \" seconds ===\")\n",
    "            print(\"Shape of dataframe: \" + str(out.shape))\n",
    "\n",
    "            print(\"\")\n",
    "            start_time = time.time()\n",
    "            print('=== Get amountgroups and daytime by Jenks natural breaks ===')\n",
    "            out = get_amountgroup(out)\n",
    "            out = hf.get_daytime(out)\n",
    "            hf.writeDb(out, pathDb = pathDb, table_name = \"TB002_DATA_CLEANED\")\n",
    "            print(\"=== Elapsed Time: \" + str(time.time() - start_time) + \" seconds ===\")\n",
    "            print(\"Shape of dataframe: \" + str(out.shape))\n",
    "        else:\n",
    "            print(\"=== Cleaned Data Table already exists - reading from DB ===\")\n",
    "            out = hf.readSqlTable(pathDb, \"TB002_DATA_CLEANED\")\n",
    "            print(\"Shape of dataframe: \" + str(out.shape))\n",
    "        \n",
    "        print(\"\")\n",
    "        start_time = time.time()\n",
    "        print(\"=== Start Feature Engineering ===\")\n",
    "        print(\"=== Get overall success rates ===\")\n",
    "        out = getOverallSR(out)\n",
    "        print(\"=== Get overall success rates for columns and column combinations\")\n",
    "        print(\"= PSP\")\n",
    "        out = combinatoric_SR(out, addColumns = [\"PSP\"])\n",
    "        print(\"= PSP x card\")\n",
    "        out = combinatoric_SR(out, addColumns = [\"PSP\", \"card\"])\n",
    "        print(\"= PSP x card x 3D_secured\")\n",
    "        out = combinatoric_SR(out, addColumns = [\"PSP\", \"card\", \"3D_secured\"])\n",
    "        print(\"= PSP x card x 3D_secured x amountgroup_word\")\n",
    "        out = combinatoric_SR(out, addColumns = [\"PSP\", \"card\", \"3D_secured\", \"amountgroup_word\"])\n",
    "        print(\"=== Elapsed Time: \" + str(time.time() - start_time) + \" seconds ===\")\n",
    "        print(\"Shape of dataframe: \" + str(out.shape))\n",
    "        \n",
    "        print(\"\")\n",
    "        start_time = time.time()\n",
    "        print(\"=== Get event window success rates for columns and column combinations\")\n",
    "        print(\"= PSP\")\n",
    "        out = combinatoric_event_window_SR(out, addColumns = [\"PSP\"])\n",
    "        print(\"= PSP x card\")\n",
    "        out = combinatoric_event_window_SR(out, addColumns = [\"PSP\", \"card\"])\n",
    "        print(\"= PSP x card x 3D_secured\")\n",
    "        out = combinatoric_event_window_SR(out, addColumns = [\"PSP\", \"card\", \"3D_secured\"])\n",
    "        print(\"= PSP x card x 3D_secured x amountgroup_word\")\n",
    "        out = combinatoric_event_window_SR(out, addColumns = [\"PSP\", \"card\", \"3D_secured\", \"amountgroup_word\"])\n",
    "        print(\"=== Elapsed Time: \" + str(time.time() - start_time) + \" seconds ===\")\n",
    "        print(\"Shape of dataframe: \" + str(out.shape))\n",
    "        \n",
    "        print(\"\")\n",
    "        start_time = time.time()\n",
    "        print(\"=== Get time window success rates for columns and column combinations\")\n",
    "        print(\"= PSP\")\n",
    "        out = combinatoric_time_window_SR(out, addColumns = [\"PSP\"])\n",
    "        print(\"= PSP x card\")\n",
    "        out = combinatoric_time_window_SR(out, addColumns = [\"PSP\", \"card\"])\n",
    "        print(\"= PSP x card x 3D_secured\")\n",
    "        out = combinatoric_time_window_SR(out, addColumns = [\"PSP\", \"card\", \"3D_secured\"])\n",
    "        print(\"= PSP x card x 3D_secured x amountgroup_word\")\n",
    "        out = combinatoric_time_window_SR(out, addColumns = [\"PSP\", \"card\", \"3D_secured\", \"amountgroup_word\"])\n",
    "        print(\"=== Elapsed Time: \" + str(time.time() - start_time) + \" seconds ===\")\n",
    "        print(\"Shape of dataframe: \" + str(out.shape))\n",
    "        \n",
    "        hf.writeDb(out, pathDb = pathDb, table_name = \"TB003_DATA_PREPARED\")\n",
    "    \n",
    "    else:\n",
    "        print(\"=== Prepared Data already exists - reading from DB ===\")\n",
    "        out = hf.readSqlTable(pathDb, \"TB003_DATA_PREPARED\")\n",
    "     \n",
    "    print(\"\")\n",
    "    print(\"============================\")\n",
    "    print(\"= time for whole pipeline: \" + str(time.time() - start_pipeline) + \" seconds\")\n",
    "    print(\"============================\")\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e420f871-1b4c-4dbf-8134-785b87f5b19b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Table does not exists ===\n",
      "=== Start raw data loading ===\n",
      "=== Table does not exists ===\n",
      "=== Table TB001_DATA_RAW created successful ===\n",
      "=== Elapsed Time: 6.2141265869140625 seconds ===\n",
      "Shape of dataframe: (50410, 7)\n",
      "=== Table does not exists ===\n",
      "\n",
      "=== Start filter rows ===\n",
      "= Half time: 152.50623726844788 seconds\n",
      "=== After cleaning up filtered data: 152 rows removed ===\n",
      "= End Time: 267.5032305717468 seconds\n",
      "=== Elapsed Time: 267.5042357444763 seconds ===\n",
      "Shape of dataframe: (27339, 12)\n",
      "\n",
      "=== Start get timestamp information ===\n",
      "=== Elapsed Time: 0.3334932327270508 seconds ===\n",
      "Shape of dataframe: (27339, 20)\n",
      "\n",
      "=== Get amountgroups and daytime by Jenks natural breaks ===\n",
      "= Jenks natural breaks are:\n",
      "[0, 99, 175, 247, 330, 10000]\n",
      "=== Table TB002_DATA_CLEANED created successful ===\n",
      "=== Elapsed Time: 1.1636948585510254 seconds ===\n",
      "Shape of dataframe: (27339, 21)\n",
      "\n",
      "=== Start Feature Engineering ===\n",
      "=== Get overall success rates ===\n",
      "=== Get overall success rates for columns and column combinations\n",
      "= PSP\n",
      "PSP_SR\n",
      "= PSP x card\n",
      "PSP_card_SR\n",
      "= PSP x card x 3D_secured\n",
      "PSP_card_3D_secured_SR\n",
      "= PSP x card x 3D_secured x amountgroup_word\n",
      "PSP_card_3D_secured_amountgroup_word_SR\n",
      "=== Elapsed Time: 1.2462365627288818 seconds ===\n",
      "Shape of dataframe: (27339, 26)\n",
      "\n",
      "=== Get event window success rates for columns and column combinations\n",
      "= PSP\n",
      "= Event window size: 5\n",
      "PSP_e5_SR\n",
      "= Event window size: 10\n",
      "PSP_e10_SR\n",
      "= Event window size: 100\n",
      "PSP_e100_SR\n",
      "= Event window size: 200\n",
      "PSP_e200_SR\n",
      "= PSP x card\n",
      "= Event window size: 5\n",
      "PSP_card_e5_SR\n",
      "= Event window size: 10\n",
      "PSP_card_e10_SR\n",
      "= Event window size: 100\n",
      "PSP_card_e100_SR\n",
      "= Event window size: 200\n",
      "PSP_card_e200_SR\n",
      "= PSP x card x 3D_secured\n",
      "= Event window size: 5\n",
      "PSP_card_3D_secured_e5_SR\n",
      "= Event window size: 10\n",
      "PSP_card_3D_secured_e10_SR\n",
      "= Event window size: 100\n",
      "PSP_card_3D_secured_e100_SR\n",
      "= Event window size: 200\n",
      "PSP_card_3D_secured_e200_SR\n",
      "= PSP x card x 3D_secured x amountgroup_word\n",
      "= Event window size: 5\n",
      "PSP_card_3D_secured_amountgroup_word_e5_SR\n",
      "= Event window size: 10\n",
      "PSP_card_3D_secured_amountgroup_word_e10_SR\n",
      "= Event window size: 100\n",
      "PSP_card_3D_secured_amountgroup_word_e100_SR\n",
      "= Event window size: 200\n",
      "PSP_card_3D_secured_amountgroup_word_e200_SR\n",
      "--- Number of missing values too large ---\n",
      "=== Elapsed Time: 9.32434892654419 seconds ===\n",
      "Shape of dataframe: (27339, 41)\n",
      "\n",
      "=== Get time window success rates for columns and column combinations\n",
      "= PSP\n",
      "= Time window size: 1h\n",
      "PSP_t1h_SR\n",
      "= Time window size: 6h\n",
      "PSP_t6h_SR\n",
      "= Time window size: 12h\n",
      "PSP_t12h_SR\n",
      "= Time window size: 24h\n",
      "PSP_t24h_SR\n",
      "= Time window size: 72h\n",
      "PSP_t72h_SR\n",
      "= PSP x card\n",
      "= Time window size: 1h\n",
      "PSP_card_t1h_SR\n",
      "--- Number of missing values too large: 0.16317348842313179 ---\n",
      "= Time window size: 6h\n",
      "PSP_card_t6h_SR\n",
      "= Time window size: 12h\n",
      "PSP_card_t12h_SR\n",
      "= Time window size: 24h\n",
      "PSP_card_t24h_SR\n",
      "= Time window size: 72h\n",
      "PSP_card_t72h_SR\n",
      "= PSP x card x 3D_secured\n",
      "= Time window size: 1h\n",
      "PSP_card_3D_secured_t1h_SR\n",
      "--- Number of missing values too large: 0.30692417425655655 ---\n",
      "= Time window size: 6h\n",
      "PSP_card_3D_secured_t6h_SR\n",
      "= Time window size: 12h\n",
      "PSP_card_3D_secured_t12h_SR\n",
      "= Time window size: 24h\n",
      "PSP_card_3D_secured_t24h_SR\n",
      "= Time window size: 72h\n",
      "PSP_card_3D_secured_t72h_SR\n",
      "= PSP x card x 3D_secured x amountgroup_word\n",
      "= Time window size: 1h\n",
      "PSP_card_3D_secured_amountgroup_word_t1h_SR\n",
      "--- Number of missing values too large: 0.7122425838545667 ---\n",
      "= Time window size: 6h\n",
      "PSP_card_3D_secured_amountgroup_word_t6h_SR\n",
      "--- Number of missing values too large: 0.25831230110830683 ---\n",
      "= Time window size: 12h\n",
      "PSP_card_3D_secured_amountgroup_word_t12h_SR\n",
      "= Time window size: 24h\n",
      "PSP_card_3D_secured_amountgroup_word_t24h_SR\n",
      "= Time window size: 72h\n",
      "PSP_card_3D_secured_amountgroup_word_t72h_SR\n",
      "=== Elapsed Time: 45.33587861061096 seconds ===\n",
      "Shape of dataframe: (27339, 57)\n",
      "=== Table TB003_DATA_PREPARED created successful ===\n",
      "\n",
      "============================\n",
      "= time for whole pipeline: 331.58519315719604 seconds\n",
      "============================\n"
     ]
    }
   ],
   "source": [
    "data_clean = applyDataCleaningFeatureEng()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "abd97f48-f784-4028-826e-16311733854c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 27339 entries, 0 to 27338\n",
      "Data columns (total 57 columns):\n",
      " #   Column                                        Non-Null Count  Dtype         \n",
      "---  ------                                        --------------  -----         \n",
      " 0   tmsp                                          27339 non-null  datetime64[ns]\n",
      " 1   country                                       27339 non-null  object        \n",
      " 2   amount                                        27339 non-null  int64         \n",
      " 3   success                                       27339 non-null  int64         \n",
      " 4   PSP                                           27339 non-null  object        \n",
      " 5   3D_secured                                    27339 non-null  int64         \n",
      " 6   card                                          27339 non-null  object        \n",
      " 7   failPrevious                                  27339 non-null  float64       \n",
      " 8   failed_Goldcard                               27339 non-null  float64       \n",
      " 9   failed_Moneycard                              27339 non-null  float64       \n",
      " 10  failed_Simplecard                             27339 non-null  float64       \n",
      " 11  failed_UK_Card                                27339 non-null  float64       \n",
      " 12  month                                         27339 non-null  object        \n",
      " 13  dayOfMonth                                    27339 non-null  int32         \n",
      " 14  weekday                                       27339 non-null  object        \n",
      " 15  weekend                                       27339 non-null  int32         \n",
      " 16  holiday                                       27339 non-null  int32         \n",
      " 17  time                                          27339 non-null  object        \n",
      " 18  daytime                                       27339 non-null  object        \n",
      " 19  minuteOfDay                                   27339 non-null  int64         \n",
      " 20  amountgroup_word                              27339 non-null  category      \n",
      " 21  overallSR                                     27339 non-null  float64       \n",
      " 22  PSP_SR                                        27339 non-null  float64       \n",
      " 23  PSP_card_SR                                   27339 non-null  float64       \n",
      " 24  PSP_card_3D_secured_SR                        27339 non-null  float64       \n",
      " 25  PSP_card_3D_secured_amountgroup_word_SR       27339 non-null  float64       \n",
      " 26  PSP_e5_SR                                     27339 non-null  float64       \n",
      " 27  PSP_e10_SR                                    27339 non-null  float64       \n",
      " 28  PSP_e100_SR                                   27339 non-null  float64       \n",
      " 29  PSP_e200_SR                                   27339 non-null  float64       \n",
      " 30  PSP_card_e5_SR                                27339 non-null  float64       \n",
      " 31  PSP_card_e10_SR                               27339 non-null  float64       \n",
      " 32  PSP_card_e100_SR                              27339 non-null  float64       \n",
      " 33  PSP_card_e200_SR                              27339 non-null  float64       \n",
      " 34  PSP_card_3D_secured_e5_SR                     27339 non-null  float64       \n",
      " 35  PSP_card_3D_secured_e10_SR                    27339 non-null  float64       \n",
      " 36  PSP_card_3D_secured_e100_SR                   27339 non-null  float64       \n",
      " 37  PSP_card_3D_secured_e200_SR                   27339 non-null  float64       \n",
      " 38  PSP_card_3D_secured_amountgroup_word_e5_SR    27339 non-null  float64       \n",
      " 39  PSP_card_3D_secured_amountgroup_word_e10_SR   27339 non-null  float64       \n",
      " 40  PSP_card_3D_secured_amountgroup_word_e100_SR  27339 non-null  float64       \n",
      " 41  PSP_t1h_SR                                    27339 non-null  float64       \n",
      " 42  PSP_t6h_SR                                    27339 non-null  float64       \n",
      " 43  PSP_t12h_SR                                   27339 non-null  float64       \n",
      " 44  PSP_t24h_SR                                   27339 non-null  float64       \n",
      " 45  PSP_t72h_SR                                   27339 non-null  float64       \n",
      " 46  PSP_card_t6h_SR                               27339 non-null  float64       \n",
      " 47  PSP_card_t12h_SR                              27339 non-null  float64       \n",
      " 48  PSP_card_t24h_SR                              27339 non-null  float64       \n",
      " 49  PSP_card_t72h_SR                              27339 non-null  float64       \n",
      " 50  PSP_card_3D_secured_t6h_SR                    27339 non-null  float64       \n",
      " 51  PSP_card_3D_secured_t12h_SR                   27339 non-null  float64       \n",
      " 52  PSP_card_3D_secured_t24h_SR                   27339 non-null  float64       \n",
      " 53  PSP_card_3D_secured_t72h_SR                   27339 non-null  float64       \n",
      " 54  PSP_card_3D_secured_amountgroup_word_t12h_SR  27339 non-null  float64       \n",
      " 55  PSP_card_3D_secured_amountgroup_word_t24h_SR  27339 non-null  float64       \n",
      " 56  PSP_card_3D_secured_amountgroup_word_t72h_SR  27339 non-null  float64       \n",
      "dtypes: category(1), datetime64[ns](1), float64(41), int32(3), int64(4), object(7)\n",
      "memory usage: 12.6+ MB\n"
     ]
    }
   ],
   "source": [
    "data_clean.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb91387-0ff9-49b7-a52e-2493545c4ec4",
   "metadata": {},
   "source": [
    "The column ```tmsp``` is included in the additionally created columns ```month```, ```dayOfMonth```, ```weekday```, ```holiday```, ```daytime``` and ```minuteOfDay```, so this column can be deleted. Furthermore the column cannot be used in any machine learning model.\n",
    "\n",
    "The columns ```daytime``` and ```time``` were created for data exploration reasons only. The columns can be completely reproduced by the column ```minuteOfDay```. So also the columns ```daytime``` and ```time``` can also be deleted.\n",
    "\n",
    "Also the columns ```amountgroup``` and ```amountgroup_word``` can be completely derived from the column ```amount``` and are artifacts from the previous data understanding steps. Highly correlated features containing redundant information can cause problems in many ML settings, so both columns will be removed.\n",
    "\n",
    "Also the feature ```failPrevious``` which is a dummy-variable to indicate if a transaction has failed previously or not.\n",
    "\n",
    "The columns ```lower```, ```upper```, ```numLower``` and ```numUpper``` were created for row-selection reasons and can also be excluded as features for the modeling phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "89769855-02c0-4254-af63-21366bf08eb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4191844817955619\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    print(data_clean[data_clean['PSP'] != \"Simplecard\"].success.mean())\n",
    "    data_clean_dropped_woTime = hf.dropColumns(data = data_clean.copy(), \n",
    "        columns = ['tmsp_hour', 'daytime', 'time', 'failedPSP', 'amountgroup_word', 'daytime', 'lower', 'upper', 'numUpper'])\n",
    "except:\n",
    "    print(\"=== Object does not exist ===\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae2c7c0-187d-4766-bbfe-77acf5cc085b",
   "metadata": {},
   "source": [
    "## Formatting and Splitting Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e5fcca-6a37-47aa-9282-e595a661ca17",
   "metadata": {},
   "source": [
    "Treebased models are particularly useful in dealing with categorical features and finding insightful breakpoints in continuous variables. From a modeling perspective it seems reasonable to achieve good modeling results with treebased models. Furthermore the dataset is not very large and contains only structured data. In a first step, the data has to be formatted in such a way, that various model implementation in Python can deal with the dataset. Most of the model implementations cannot deal with categorical features. This means all categorical and ordinal features in the dataset have to be enconded to numerical features. This can be achieved by One-Hot or Label-Encoding.\n",
    "\n",
    "One-Hot encoding is useful, when the categorical variables do not have too many unique values. Label-Encoding can be useful, when the categorical variable have many unique values. Label-Encoding is also used for ordinal variables. So the cardinality of the categorical variables in the dataset has to be inspected first. The categorical features in the dataset are:\n",
    "* country\n",
    "* success\n",
    "* PSP\n",
    "* 3D_secured\n",
    "* card\n",
    "* month\n",
    "* dayOfMonth\n",
    "* weekday\n",
    "* weekend\n",
    "* holiday\n",
    "\n",
    "From those variables the following variables are already in a numeric format and can be used in ML models:\n",
    "* success\n",
    "* 3D_secured\n",
    "* dayOfMonth\n",
    "* weekend\n",
    "* holiday\n",
    "\n",
    "The cardinality of the remaining variables are:\n",
    "* PSP: 4\n",
    "* card: 3\n",
    "* month: 2\n",
    "* weekday: 7\n",
    "\n",
    "This shows that the yet to be formatted variables have a low cardinality and will be One-Hot-transformed to become useful in terms of modeling purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e735b8df-3f11-4ac9-854a-2a6f530b199e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatData(data, columns = ['PSP', 'card', 'month', 'weekday', 'country']):\n",
    "    out = data.copy()\n",
    "    \n",
    "    out = pd.get_dummies(out, columns=columns, drop_first=True)\n",
    "    print(\"=== Number of missing values ===\")\n",
    "    print(out.isna().sum().sum())\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e58c8329-4556-4a8b-9257-b3fab93ea6ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Number of missing values ===\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "data_formatted_time = formatData(data_clean_dropped_woTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5eeb97f0-6e3a-4c9b-8493-fb96d1af41a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getColumnsToScale(data):\n",
    "    from pandas.api.types import is_numeric_dtype\n",
    "    out = []\n",
    "    for column in data.copy().columns:\n",
    "        if is_numeric_dtype(data.copy()[column]):\n",
    "            if data.copy()[column].max() > 1:\n",
    "                out.append(column)\n",
    "        else:\n",
    "            print(\"Column \" + column + \" is not numeric\")\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6aec9398-badd-407b-85d1-66a369c577aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def applyRandomSplitting(data, train_size = 0.7, test_size = 0.15, validate_size = 0.15):\n",
    "    applyData = data.copy()\n",
    "    y = applyData['success']\n",
    "    X = hf.dropColumns(applyData, columns = [\"success\"])\n",
    "    \n",
    "    scale_columns = getColumnsToScale(applyData)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=test_size + validate_size, random_state=1977)\n",
    "    X_test, X_validate, y_test, y_validate = train_test_split(X_test, y_test, stratify=y_test, \n",
    "                                                              test_size=test_size/(test_size + validate_size), \n",
    "                                                              random_state=1977)\n",
    "    scaler = MinMaxScaler()\n",
    "    X_train[scale_columns] = scaler.fit_transform(X_train[scale_columns])\n",
    "    X_test[scale_columns] = scaler.transform(X_test[scale_columns])\n",
    "    X_validate[scale_columns] = scaler.transform(X_validate[scale_columns])\n",
    "    \n",
    "    print(\"= Success rate in y_train: \" + str(y_train.sum()/len(y_train)))\n",
    "    print(\"= Success rate in y_validate: \" + str(y_validate.mean()))\n",
    "    print(\"= Success rate in y_test: \" + str(y_test.mean()))\n",
    "    \n",
    "    return (X, y, X_train, y_train, X_validate, y_validate, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0986b296-d18c-4fad-a8a2-5f00f49dc593",
   "metadata": {},
   "outputs": [],
   "source": [
    "def applyTimeSplitting(data, train_size = 0.7, test_size = 0.15, validate_size = 0.15, time_col = \"tmsp\"):\n",
    "    applyData = data.copy()\n",
    "    # applyData = applyData.sort_values(by = [time_col], ascending = True)\n",
    "    length = len(applyData)\n",
    "    \n",
    "    train_length = int(np.round(length*train_size))\n",
    "    test_length = int(length - train_length)\n",
    "    validate_length = int(np.round(test_length * (validate_size/(validate_size + test_size))))\n",
    "    test_length = int(test_length - validate_length)\n",
    "    \n",
    "    y = applyData['success']\n",
    "    X = hf.dropColumns(applyData, columns = [\"success\", time_col])\n",
    "    \n",
    "    assert (test_length + validate_length + train_length) == length, f\"number expected: {length}, got: {test_length + validate_length + train_length}\"\n",
    "    \n",
    "    X_train = X.copy().iloc[:train_length, :]\n",
    "    y_train = y.copy().iloc[:train_length]\n",
    "    X_validate = X.copy().iloc[train_length:(train_length + validate_length), :]\n",
    "    y_validate = y.copy().iloc[train_length:(train_length + validate_length)]\n",
    "    X_test = X.copy().iloc[(train_length + validate_length):, :]\n",
    "    y_test = y.copy().iloc[(train_length + validate_length):]\n",
    "    \n",
    "    assert (len(X_train) + len(X_validate) + len(X_test)) == length, f\"number expected: {length}, got: {(len(X_train) + len(X_validate) + len(X_test))}\"\n",
    "    \n",
    "    scale_columns = getColumnsToScale(X)\n",
    "    scaler = MinMaxScaler()\n",
    "    X_train[scale_columns] = scaler.fit_transform(X_train[scale_columns])\n",
    "    parameters = hf.loadPickle('./data/parameters.pkl')\n",
    "    parameters[\"scaler\"] = scaler\n",
    "    parameters[\"scale_columns\"] = scale_columns\n",
    "    hf.writePickle('./data/parameters.pkl', parameters)\n",
    "    X_test[scale_columns] = scaler.transform(X_test[scale_columns])\n",
    "    X_validate[scale_columns] = scaler.transform(X_validate[scale_columns])\n",
    "    \n",
    "    print(\"= Success rate in y_train: \" + str(y_train.mean()))\n",
    "    print(\"= Success rate in y_validate: \" + str(y_validate.mean()))\n",
    "    print(\"= Success rate in y_test: \" + str(y_test.mean()))\n",
    "    \n",
    "    return (X, y, X_train, y_train, X_validate, y_validate, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9955829e-ec67-4520-9140-bda31dd95a3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "= Success rate in y_train: 0.38266185922558393\n",
      "= Success rate in y_validate: 0.35137771275298707\n",
      "= Success rate in y_test: 0.3550353572299439\n",
      "=== Table does not exists ===\n",
      "=== Table X created successful ===\n",
      "=== Table y created successful ===\n",
      "=== Table does not exists ===\n",
      "=== Table X_train created successful ===\n",
      "=== Table y_train created successful ===\n",
      "=== Table does not exists ===\n",
      "=== Table X_validate created successful ===\n",
      "=== Table y_validate created successful ===\n",
      "=== Table does not exists ===\n",
      "=== Table X_test created successful ===\n",
      "=== Table y_test created successful ===\n"
     ]
    }
   ],
   "source": [
    "X, y, X_train, y_train, X_validate, y_validate, X_test, y_test = applyTimeSplitting(data_formatted_time)\n",
    "if not hf.checkIfTableDbExists('./data/PSP_Data.sqlite', \"X\"):\n",
    "    hf.writeDb(X, pathDb = './data/PSP_Data.sqlite', table_name = \"X\")\n",
    "    hf.writeDb(y, pathDb = './data/PSP_Data.sqlite', table_name = \"y\")\n",
    "if not hf.checkIfTableDbExists('./data/PSP_Data.sqlite', \"X_train\"):\n",
    "    hf.writeDb(X_train, pathDb = './data/PSP_Data.sqlite', table_name = \"X_train\")\n",
    "    hf.writeDb(y_train, pathDb = './data/PSP_Data.sqlite', table_name = \"y_train\")\n",
    "if not hf.checkIfTableDbExists('./data/PSP_Data.sqlite', \"X_validate\"):\n",
    "    hf.writeDb(X_validate, pathDb = './data/PSP_Data.sqlite', table_name = \"X_validate\")\n",
    "    hf.writeDb(y_validate, pathDb = './data/PSP_Data.sqlite', table_name = \"y_validate\")\n",
    "if not hf.checkIfTableDbExists('./data/PSP_Data.sqlite', \"X_test\"):\n",
    "    hf.writeDb(X_test, pathDb = './data/PSP_Data.sqlite', table_name = \"X_test\")\n",
    "    hf.writeDb(y_test, pathDb = './data/PSP_Data.sqlite', table_name = \"y_test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6e8a10-22e9-4cf0-8c61-e1bf46ba4011",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3469c20c-9eac-4cc5-a06e-605a13b3001b",
   "metadata": {},
   "source": [
    "<p>Bygari, R., Gupta, A., Raghuvanshi, S., Bapna, A., & Sahu, B. (2021). An AI-powered Smart Routing Solution for Payment Systems. 2026–2033. https://doi.org/10.1109/BigData52589.2021.9671961</p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
