{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ba904b9-92c4-4a50-9925-f18870a18b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jenkspy import JenksNaturalBreaks\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import pickle\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import PredefinedSplit\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c805d3a-6c8c-4a94-90f1-bdfea26352de",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5d201f9b-0199-4845-a8ac-7546b0c2ef37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadPickle(path):\n",
    "    out = pickle.load(open(path, 'rb'))\n",
    "    return out\n",
    "\n",
    "def writePickle(path, object):\n",
    "    pickle_out = open(path, \"wb\")\n",
    "    pickle.dump(object, pickle_out)\n",
    "    pickle_out.close()\n",
    "\n",
    "def get_y(data, groups=[\"card\"]):\n",
    "    y1 = data.groupby(groups).aggregate({'tmsp': 'count'}).rename(columns={'tmsp': 'count'}).reset_index()\n",
    "    y2 = data[data[\"success\"] == 1].groupby(groups).aggregate({'tmsp': 'count'}).rename(columns={\"tmsp\": \"count_success\"}).reset_index()\n",
    "    \n",
    "    y = y1.merge(y2, on = groups)\n",
    "    y[\"success_rate\"] = y['count_success']/y['count']\n",
    "    \n",
    "    return y\n",
    "\n",
    "def generateDataFromFile():\n",
    "    if (\n",
    "        (os.path.isfile('./data/X_train.csv')) & (os.path.isfile('./data/X_test.csv')) & \n",
    "        (os.path.isfile('./data/y_train.csv')) & (os.path.isfile('./data/y_test.csv')) &\n",
    "        (os.path.isfile('./data/y_validate.csv')) & (os.path.isfile('./data/X_validate.csv'))\n",
    "    ):\n",
    "        print(\"generate from files\")\n",
    "        X = pd.read_csv('./data/X.csv', index_col = 'index')\n",
    "        X.index.name = None\n",
    "        y = pd.read_csv('./data/y.csv', index_col = 'index')\n",
    "        y.index.name = None\n",
    "        X_train = pd.read_csv('./data/X_train.csv', index_col = 'index')\n",
    "        X_train.index.name = None\n",
    "        X_test = pd.read_csv('./data/X_test.csv', index_col = 'index')\n",
    "        X_test.index.name = None\n",
    "        X_validate = pd.read_csv('./data/X_validate.csv', index_col = 'index')\n",
    "        X_validate.index.name = None\n",
    "        y_train = pd.read_csv('./data/y_train.csv', index_col = 'index')\n",
    "        y_train.index.name = None\n",
    "        y_test = pd.read_csv('./data/y_test.csv', index_col = 'index')\n",
    "        y_test.index.name = None\n",
    "        y_validate = pd.read_csv('./data/y_validate.csv', index_col = 'index')\n",
    "        y_validate.index.name = None\n",
    "        \n",
    "        print(\"=== all data loaded from files ===\")\n",
    "        \n",
    "        return (X, y, X_train, y_train, X_validate, y_validate, X_test, y_test)\n",
    "    else:\n",
    "        print(\"skip the step because not all data is prepared yet\")\n",
    "        \n",
    "        return (None, None, None, None, None, None, None, None)\n",
    "\n",
    "def correlationFiltering(X_train, threshold = 0.75, figsize = 10):\n",
    "    plt.figure(figsize=(figsize, figsize))\n",
    "    sns.heatmap(X_train.corr().round(2), annot=False)\n",
    "    plt.show()\n",
    "    \n",
    "    # create correlation matrix\n",
    "    corrMatrix = X_train.corr().abs()\n",
    "    # get upper triangle\n",
    "    upperCorrMatrix = corrMatrix.where(\n",
    "        np.triu(np.ones(corrMatrix.shape), k=1).astype(np.bool_))\n",
    "    uniqueCorrPairs = upperCorrMatrix.unstack().dropna()\n",
    "    sortedCorrPairs = uniqueCorrPairs.sort_values(ascending = False)\n",
    "    # identify all paird with correlation greater than threshold\n",
    "    pairsToFilter = sortedCorrPairs[sortedCorrPairs > threshold]\n",
    "    toRemove = []\n",
    "    for pair in pairsToFilter.index:\n",
    "        # calculate average correlation between A and other variables and B with other variables\n",
    "        a = pair[0]\n",
    "        a_avg = corrMatrix[a].mean()\n",
    "        b = pair[1]\n",
    "        b_avg = corrMatrix[b].mean()\n",
    "        # if A has a larger average correlation, remove it, otherwise remove B\n",
    "        if a_avg > b_avg:\n",
    "            toRemove.append(a)\n",
    "        else:\n",
    "            toRemove.append(b)\n",
    "\n",
    "    return list(set(toRemove))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "674b5e38-936a-43fd-b905-580848881618",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generate from files\n",
      "=== all data loaded from files ===\n"
     ]
    }
   ],
   "source": [
    "X, y, X_train, y_train, X_validate, y_validate, X_test, y_test = generateDataFromFile()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7a7265-aabf-46ed-a114-dc1aff233cf7",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "This is the point at which your hard work begins to pay off. The data you spent time preparing are brought into the analysis tools in IBM SPSS Modeler, and the results begin to shed some light on the business problem posed during Business Understanding. Modeling is usually conducted in multiple iterations. Typically, data miners run several models using the default parameters and then fine-tune the parameters or revert to the data preparation phase for manipulations required by their model of choice. It is rare for an organization's data mining question to be answered satisfactorily with a single model and a single execution. This is what makes data mining so interesting--there are many ways to look at a given problem, and IBM SPSS Modeler offers a wide variety of tools to help you do so."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ebf767-a8ee-410a-8f46-3369d7595c2d",
   "metadata": {},
   "source": [
    "## Selecting Modeling Technique\n",
    "Although you may already have some idea about which types of modeling are most appropriate for your organization's needs, now is the time to make some firm decisions about which ones to use. Determining the most appropriate model will typically be based on the following considerations: \n",
    "* The data types available for mining. For example, are the fields of interest categorical (symbolic)? \n",
    "* Your data mining goals. Do you simply want to gain insight into transactional data stores and unearth interesting purchase patterns? Or do you need to produce a score indicating, for example, propensity to default on a student loan? \n",
    "* Specific modeling requirements. Does the model require a particular data size or type? Do you need a model with easily presentable results? \n",
    "\n",
    "For more information on the model types in IBM SPSS Modeler and their requirements, see the IBM SPSS Modeler documentation or online Help.\n",
    "\n",
    "### Choosing the Right Modeling Techniques \n",
    "\n",
    "Many modeling techniques are available in IBM SPSS Modeler. Frequently, data miners use more than one to approach the problem from a number of directions. Task List When deciding on which model(s) to use, consider whether the following issues have an impact on your choices\n",
    "\n",
    "* Does the model require the data to be split into test and training sets? • Do you have enough data to produce reliable results for a given model? \n",
    "* Does the model require a certain level of data quality? Can you meet this level with the current data? \n",
    "* Are your data the proper type for a particular model? If not, can you make the necessary conversions using data manipulation nodes? \n",
    "\n",
    "For more information on the model types in IBM SPSS Modeler and their requirements, see the IBM SPSS Modeler documentation or online Help. \n",
    "\n",
    "### Modeling Assumptions \n",
    "\n",
    "As you begin to narrow down your modeling tools of choice, take notes on the decision-making process. Document any data assumptions as well as any data manipulations made to meet the model's requirements. For example, both the Logistic Regression and Neural Net nodes require the data types to be fully instantiated (data types are known) before execution. This means you will need to add a Type node to the stream and execute it to run the data through before building and running a model. Similarly, predictive models, such as C5.0, may benefit from rebalancing the data when predicting rules for rare events. When making this type of prediction, you can often get better results by inserting a Balance node into the stream and feeding the more balanced subset into the model. Be sure to document these types of decisions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f98a8ebf-c281-4bab-99ab-29e8c599c94f",
   "metadata": {},
   "source": [
    "Most of the data work already has been done in the previous data preparation step:\n",
    "* all continuous data is scaled between 0 and 1\n",
    "* missing data during feature engineering has already been imputed\n",
    "* categorical data has been transformed by One-Hot Encoding to dummy variables, this means all data in ```X_train```, ```X_valid``` and ```X_test``` is already in a numerical format\n",
    "* The final training data has 19,243 observations and 62 features. This means the number of predictors is much lower than the number of samples, which implies that also linear modeling techniques like multiple linear regression, logistic regression, and linear discriminant analysis can be used for the problem at hand (Kuhn & Johnson, 2019).\n",
    "* For models that assume that there is no extreme multicollinearity present in the data, meaning that the between-predictor correlation is not too high (Kuhn & Johnson, 2013), like linear or logistic regression, a correlation-based filter method has already been implemented. This method was proposed by Kuhn & Johnson (2013, p. 47).\n",
    "\n",
    "In this case we have a binary outcome variable coded as ```success``` (1) or ```no success```(0). This means the problem at hand is a binary classification problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707c0cf1-b308-4c55-b796-a7c50929ae60",
   "metadata": {},
   "source": [
    "## Generating a Test Design \n",
    "As a final step before actually building the model, you should take a moment to consider again how the model's results will be tested. There are two parts to generating a comprehensive test design: \n",
    "* Describing the criteria for \"goodness\" of a model \n",
    "* Defining the data on which these criteria will be tested \n",
    "\n",
    "A model's goodness can be measured in several ways. For supervised models, such as C5.0 and C&R Tree, measurements of goodness typically estimate the error rate of a particular model. For unsupervised models, such as Kohonen cluster nets, measurements may include criteria such as ease of interpretation, deployment, or required processing time. Remember, model building is an iterative process. This means that you will typically test the results of several models before deciding on the ones to use and deploy. \n",
    "\n",
    "### Writing a Test Design \n",
    "\n",
    "The test design is a description of the steps you will take to test the models produced. Because modeling is an iterative process, it is important to know when to stop adjusting parameters and try another method or model. Task List When creating a test design, consider the following questions: \n",
    "* What data will be used to test the models? Have you partitioned the data into train/test sets? (This is a commonly used approach in modeling.) \n",
    "* How might you measure the success of supervised models (such as C5.0)?\n",
    "* How might you measure the success of unsupervised models (such as Kohonen cluster nets)?\n",
    "* How many times are you willing to rerun a model with adjusted settings before attempting another type of model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7d4338-696d-49a7-a648-5bed91af670a",
   "metadata": {},
   "source": [
    "Assuming that the provided data set are predictions from the company´s current manual and rule-based routing logic the overall success rate can be interpreted by the precision of the current routing logic. This means:\n",
    "\n",
    "$Precision = \\frac{TP}{TP + FP}$\n",
    "\n",
    "This means that every transaction is interpreted as a ```positive``` transaction by the routing-logic. ```No success``` would then be interpreted as **FP** and ```success``` as **TP**.\n",
    "\n",
    "From this the precision for the routing logic would be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b2ca04ed-f309-4a37-ace7-866130620b65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "success    0.371758\n",
       "dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2fe3db0-0e1b-45bb-b7d6-1fa5bbea7109",
   "metadata": {},
   "source": [
    "If we further assume that the PSP Simplecard is treated as a fall-back option by the current routing logic and would be treated as a negative prediction, the precision would be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "32375b42-884f-4f01-8fe7-6344ddeea2a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "success    0.416793\n",
       "dtype: float64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[X['PSP_Simplecard'] != 1].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5278157-2dd4-4294-a699-645cdb2a67be",
   "metadata": {},
   "source": [
    "This means the first baseline-metric to beat by the ML workflow should be a better Precision than 0.42 as the final ML model on hold-out test data. A second baseline-metric to beat would be the accuracy of a majority-vote prediction (in this case for the whole dataset), which would be 1-0.37 = 0.63. This basic baseline-procedure is also proposed by Kuhn & Johnson (2013). Because both described heuristic benchmark ... Lakshmanan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633b6b18-6b39-4992-a0ec-778114e9e349",
   "metadata": {},
   "source": [
    "## Building the Models \n",
    "\n",
    "At this point, you should be well prepared to build the models you've spent so long considering. Give yourself time and room to experiment with a number of different models before making final conclusions. Most data miners typically build several models and compare the results before deploying or integrating them. \n",
    "\n",
    "In order to track your progress with a variety of models, be sure to keep notes on the settings and data used for each model. This will help you to discuss the results with others and retrace your steps if necessary. At the end of the model-building process, you'll have three pieces of information to use in data mining decisions:\n",
    "* Parameter settings include the notes you take on parameters that produce the best results.\n",
    "* The actual models produced.\n",
    "* Descriptions of model results, including performance and data issues that occurred during the execution of the model and exploration of its results.\n",
    "\n",
    "### Parameter Settings \n",
    "Most modeling techniques have a variety of parameters or settings that can be adjusted to control the modeling process. For example, decision trees can be controlled by adjusting tree depth, splits, and a number of other settings. Typically, most people build a model first using the default options and then refine parameters during subsequent sessions. \n",
    "\n",
    "Once you have determined the parameters that produce the most accurate results, be sure to save the stream and generated model nodes. Also, taking notes on the optimal settings can help when you decide to automate or rebuild the model with new data. \n",
    "\n",
    "### Running the Models \n",
    "\n",
    "In IBM SPSS Modeler, running models is a straightforward task. Once you've inserted the model node into the stream and edited any parameters, simply execute the model to produce viewable results. Results appear in the Generated Models navigator on the right side of the workspace. You can right-click a model to browse the results. For most models, you can insert the generated model into the stream to further evaluate and deploy the results. Models can be also be saved in IBM SPSS Modeler for easy reuse. \n",
    "\n",
    "### Model description \n",
    "\n",
    "When examining the results of a model, be sure to take notes on your modeling experience. You can store notes with the model itself using the node annotations dialog box or the project tool. Task list For each model, record information such as:\n",
    "* Can you draw meaningful conclusions from this model?\n",
    "* Are there new insights or unusual patterns revealed by the model?\n",
    "* Were there execution problems for the model? How reasonable was the processing time?\n",
    "* Did the model have difficulties with data quality issues, such as a high number of missing values?\n",
    "* Were there any calculation inconsistencies that should be noted?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf1ce3e6-7cee-44b2-b422-7f147cf7963a",
   "metadata": {},
   "source": [
    "## Assessing the Model \n",
    "\n",
    "Now that you have a set of initial models, take a closer look at them to determine which are accurate or effective enough to be final. Final can mean several things, such as \"ready to deploy\" or \"illustrating interesting patterns.\" Consulting the test plan that you created earlier can help to make this assessment from your organization's point of view. \n",
    "\n",
    "### Comprehensive Model Assessment \n",
    "\n",
    "For each model under consideration, it is a good idea to make a methodical assessment based on the criteria generated in your test plan. Here is where you may add the generated model to the stream and use evaluation charts or analysis nodes to analyze the effectiveness of the results. You should also consider whether the results make logical sense or whether they are too simplistic for your business goals (for example, a sequence that reveals purchases such as wine > wine > wine). \n",
    "\n",
    "Once you've made an assessment, rank the models in order based on both objective (model accuracy) and subjective (ease of use or interpretation of results) criteria. \n",
    "\n",
    "Task List\n",
    "* Using the data mining tools in IBM SPSS Modeler, such as evaluation charts, analysis nodes, or crossvalidation charts, evaluate the results of your model.\n",
    "* Conduct a review of the results based on your understanding of the business problem. Consult data analysts or other experts who may have insight into the relevance of particular results.\n",
    "* Consider whether a model's results are easily deployable. Does your organization require that results be deployed over the Web or sent back to the data warehouse?\n",
    "* Analyze the impact of results on your success criteria. Do they meet the goals established during the business understanding phase? \n",
    "\n",
    "If you were able to address the above issues successfully and believe that the current models meet your goals, it's time to move on to a more thorough evaluation of the models and a final deployment. Otherwise, take what you've learned and rerun the models with adjusted parameter settings.\n",
    "\n",
    "### Keeping Track of Revised Parameters \n",
    "\n",
    "Based on what you've learned during model assessment, it's time to have another look at the models. You have two options here:\n",
    "* Adjust the parameters of existing models.\n",
    "* Choose a different model to address your data mining problem. \n",
    "\n",
    "In both cases, you'll be returning to the building models task and iterate until the results are successful. Don't worry about repeating this step. It is extremely common for data miners to evaluate and rerun models several times before finding one that meets their needs. This is a good argument for building several models at once and comparing the results before adjusting the parameters for each."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
